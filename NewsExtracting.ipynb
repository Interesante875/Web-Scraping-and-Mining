{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkWvcRk2LfmB"
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import threading\n",
        "import logging\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTXMtrInsEdC"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import threading\n",
        "import logging\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0HMFM7kjxhC"
      },
      "source": [
        "\n",
        "# targets = ['pcr', 'virus', 'ventilator', 'mysejahtera', 'antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'moderna', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb1cZT_Vkl7_"
      },
      "source": [
        "The star\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pdLaa5pRil"
      },
      "source": [
        "class The_Star_Crawler:\n",
        "    def __init__(self, targets, sleeptime, directory, verbose):\n",
        "        if isinstance(targets, list):\n",
        "            self.targets = targets\n",
        "        else:\n",
        "            self.targets = [targets]\n",
        "        self.sleeptime = sleeptime\n",
        "        self.verbose = verbose\n",
        "        self.directory = directory\n",
        "\n",
        "\n",
        "    def obtainPageNumber(self, target):\n",
        "        url = f'https://www.thestar.com.my/search?q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "        time.sleep(0.015)\n",
        "        source = requests.get(url).text\n",
        "        soup = BeautifulSoup(source, 'lxml')\n",
        "        regex = r\"a*[0-9]+\"\n",
        "        page_navs = soup.find_all(id=\"slcontent_0_ThisPageRowCounts\")\n",
        "        largest_query = 1\n",
        "        for page_nav in page_navs:\n",
        "            page_list = page_nav.text\n",
        "            page_iter = re.finditer(regex, page_list)\n",
        "            for match in page_iter:\n",
        "                largest_query = match.group(0)\n",
        "        largest_page = math.ceil(float(largest_query)/30)\n",
        "        print(f'The total number of query result for the target word {target} is {largest_query}, with {largest_page} pages.')\n",
        "\n",
        "        return largest_page\n",
        "\n",
        "    def create_project(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'Creating project {directory}')\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    def crawl(self, largest_page, target, sleeptime):\n",
        "        csv_file = open(f'{self.directory}/{target}.csv', 'w')\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        if (self.verbose):\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link', 'content'])\n",
        "        else:\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "        for page in range(1, largest_page):\n",
        "            print(f'Crawling page {page} of target {target}')\n",
        "            time.sleep(0.015)\n",
        "            url = f'https://www.thestar.com.my/search?pgno={page}&q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "            source = requests.get(url).text\n",
        "            soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "            topics = soup.find_all(class_=\"row list-listing\")\n",
        "\n",
        "            for topic in topics:\n",
        "                headline = topic.div.h2.a.text\n",
        "                time_meta = topic.div.span.text\n",
        "                date_posted = time_meta.split('|')\n",
        "                date_posted = [item.strip() for item in date_posted]\n",
        "                reference = topic.div.h2.a.get('href')\n",
        "\n",
        "                if (self.verbose):\n",
        "                    content_url = reference\n",
        "                    time.sleep(0.015)\n",
        "                    content_source = requests.get(content_url).text\n",
        "                    content_soup = BeautifulSoup(content_source, 'lxml')\n",
        "\n",
        "                    content_topics = content_soup.find_all('p')\n",
        "                    news_body = []\n",
        "                    ##print(content_topics)\n",
        "\n",
        "                    for content_topic in content_topics:\n",
        "                        content_body = content_topic.text\n",
        "                        #print(content_body)\n",
        "                        news_body.append(content_body)\n",
        "                    del news_body[-8:]\n",
        "                    news_content = \"\\n\".join(news_body).strip()\n",
        "\n",
        "                    time.sleep(0.4)\n",
        "\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference, news_content])\n",
        "                else:\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "            time.sleep(sleeptime)\n",
        "\n",
        "        csv_file.close()\n",
        "\n",
        "\n",
        "    def run(self, target):\n",
        "        pageNo = self.obtainPageNumber(target)\n",
        "        self.crawl(pageNo, target, self.sleeptime)\n",
        "\n",
        "    def multirun(self):\n",
        "\n",
        "        self.create_project(self.directory)\n",
        "        threads = []\n",
        "        for index, target in enumerate(self.targets):\n",
        "            logging.info(\"Main    : create and start thread %d.\", index)\n",
        "\n",
        "            t = threading.Thread(target=self.run, args = (target, ))\n",
        "            threads.append(t)\n",
        "            t.start()\n",
        "        for thr in threads:\n",
        "\n",
        "            thr.join()\n",
        "\n",
        "\n",
        "\n",
        "        #!zip -r /content/TheStar.zip /content/TheStar\n",
        "        #from google.colab import files\n",
        "        #files.download(\"/content/TheStar.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTGjKVo9pV1V"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    targets = [['mandate', 'pfizer', 'moderna', 'sinovac', 'astrazeneca','khairy','sinopharm', 'mrna'],\n",
        "               ['mco','emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing'],\n",
        "               ['mask', 'masking', 'ismail', 'recovery', 'lockdown']]\n",
        "\n",
        "    for target in targets:\n",
        "        web_crawler = The_Star_Crawler(target, 1, 'TheStar', True)\n",
        "        web_crawler.multirun()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv924lO756Bd"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/TheStar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu1Yr-QTkoAj"
      },
      "source": [
        "Malay mail\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDN5DiPXkpEO"
      },
      "source": [
        "class MalayMailScraper:\n",
        "\n",
        "    def __init__(self, base_url, relevancy, year, month, slept, sleeptime_mean, sleeptime_var, directory, verbose):\n",
        "        self.base_url = base_url\n",
        "        self.relevancy = relevancy\n",
        "        self.year = year\n",
        "        self.month = month\n",
        "        self.slept = slept\n",
        "        self.sleeptime_mean = sleeptime_mean\n",
        "        self.sleeptime_var = sleeptime_var\n",
        "        self.directory = directory\n",
        "        self.verbose = verbose\n",
        "        if (self.verbose):\n",
        "            self.results = pd.DataFrame(data = None, columns = ['Title', 'Year', 'Month', 'Link', 'Content'])\n",
        "        else:\n",
        "            self.results = pd.DataFrame(data = None, columns = ['Title', 'Year', 'Month', 'Link'])\n",
        "\n",
        "    def create_project(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'Creating project {directory}')\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    def fetch(self, base_url, type, year, month, page, slept, sleeptime_mean, sleeptime_var):\n",
        "        # Make HTTP GET request\n",
        "        url = f'{base_url}news/{type}/{year}/{month}?page={page}'\n",
        "        print(url)\n",
        "        response = requests.get(url)\n",
        "        print('type: %s, HTTP GET request to URL: %s | Status code: %s' % (type, response.url, response.status_code))\n",
        "\n",
        "        # Return HTTP response\n",
        "        if(slept):\n",
        "            time.sleep(sleeptime_mean + random.random()*sleeptime_var)\n",
        "        return response\n",
        "\n",
        "    def subfetchParse(self, url, slept, sleeptime_mean, sleeptime_var):\n",
        "        corpus = ''\n",
        "        if(slept):\n",
        "            time.sleep(sleeptime_mean/10 + random.random()*sleeptime_var/10)\n",
        "        response = requests.get(url)\n",
        "        content = BeautifulSoup(response.text, 'lxml')\n",
        "        article_data = content.find_all('article')\n",
        "        if article_data:\n",
        "            paras = article_data.p.text\n",
        "            corpus = ' '.join(paras)\n",
        "        return corpus\n",
        "\n",
        "    def parse(self, html, year, month, relevancy,  slept, sleeptime_mean, sleeptime_var, verbose):\n",
        "        '''Parses response's text and extract data from it'''\n",
        "\n",
        "        # Parse content\n",
        "        content = BeautifulSoup(html, 'lxml')\n",
        "        # Extract data\n",
        "        reference = content.find_all('a', {'class':'py-2 d-block'})\n",
        "        if reference:\n",
        "            hyperlink = [link.get('href') for link in reference]\n",
        "            title = [link.text.lower() for link in reference]\n",
        "\n",
        "            relevancy_dict = self.relevancy_check(hyperlink, title)\n",
        "            title_copied = relevancy_dict['titles']\n",
        "            hyperlink_copied = relevancy_dict['links']\n",
        "\n",
        "            if (verbose):\n",
        "                # Loop over the number of entries\n",
        "                for index in range(0, len(title_copied)):\n",
        "                    # Append extracted data to results list\n",
        "                    self.results.append({\n",
        "                        'Title': title_copied[index],\n",
        "                        'Year': year,\n",
        "                        'Month': month,\n",
        "                        'Link': hyperlink_copied[index]\n",
        "                    }, ignore_index=True)\n",
        "            else:\n",
        "                for index in range(0, len(title_copied)):\n",
        "                    # Append extracted data to results list\n",
        "                    corpus = self.subfetchParse(hyperlink_copied[index], slept, sleeptime_mean, sleeptime_var)\n",
        "\n",
        "                    self.results.append({\n",
        "                        'Title': title_copied[index],\n",
        "                        'Year': year,\n",
        "                        'Month': month,\n",
        "                        'Link': hyperlink_copied[index],\n",
        "                        'Content': corpus\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def relevancy_check(self, relevancy, hyperlink_lst, title_lst):\n",
        "        hyperlinks = [link for link in hyperlink_lst]\n",
        "        titles = [title.lower() for title in title_lst]\n",
        "        delno = 0\n",
        "\n",
        "        #check for relevancy\n",
        "        for index in range(len(titles)):\n",
        "            tokens = title_lst[index].split()\n",
        "            relevant = False\n",
        "            for check in relevancy:\n",
        "                if (check in tokens):\n",
        "                    relevant = True\n",
        "                    break\n",
        "            if not relevant:\n",
        "                del titles[index-delno]\n",
        "                del hyperlinks[index-delno]\n",
        "                delno = delno + 1\n",
        "            relevant = False\n",
        "\n",
        "        return {\n",
        "            'links': hyperlinks,\n",
        "            'titles': titles,\n",
        "            'queries': len(titles) - delno\n",
        "        }\n",
        "\n",
        "    def write_file(self, directory, excel_filename):\n",
        "        '''Writes scpared results to CSV file'''\n",
        "\n",
        "        # Check results list in not empty\n",
        "        if len(self.results):\n",
        "            print(f'Writing results to {directory}/{excel_filename}')\n",
        "\n",
        "            self.results.to_excel(f'/content/{directory}/{excel_filename}')\n",
        "\n",
        "            print('Done')\n",
        "\n",
        "    def save_file(self, directory, excel_filename, verbose):\n",
        "        !zip -r /content/MalayMail.zip /content/MalayMail\n",
        "        from google.colab import files\n",
        "        files.download(f\"/content/MalayMail.zip\")\n",
        "\n",
        "    def multirun(self):\n",
        "        types = ['malaysia', 'world']\n",
        "        years = ['2020', '2021']\n",
        "        months = ['01', '02' ,'03','04','05','06','07','08','09','10','11','12']\n",
        "\n",
        "        self.create_project(self.directory)\n",
        "        # Loop over the range of pages to scrape\n",
        "        for type in types:\n",
        "            for year in years:\n",
        "                for month in months:\n",
        "                    t = threading.Thread(target=self.run, args = (self.base_url, type, year, month,\n",
        "                                                                  self.slept, self.sleeptime_mean, self.sleeptime_var,\n",
        "                                                                  self.verbose,))\n",
        "                    t.start()\n",
        "        t.join()\n",
        "        t.active_threads()\n",
        "\n",
        "        self.write_file(self.directory, 'MalayMail', self.verbose)\n",
        "        self.save_file (self.directory, 'MalayMail', self.verbose)\n",
        "\n",
        "    def run(self, base_url, type, year, month, slept, sleeptime_mean, sleeptime_var, verbose):\n",
        "        content_exist = True\n",
        "        page = 1\n",
        "        while content_exist:\n",
        "            # Make HTTP GET request\n",
        "            response = self.fetch(base_url, type, year, month, slept, sleeptime_mean, sleeptime_var)\n",
        "            # Parse content\n",
        "            content_exist = self.parse(response.text, year, month, relevancy,\n",
        "                                       slept, sleeptime_mean, sleeptime_var, verbose)\n",
        "            page = page + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16iyaCWZk5SL"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    random.seed(datetime.now())\n",
        "    web_crawler = MalayMailScraper()\n",
        "    web_crawler.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import dataclasses\n",
        "import datetime\n",
        "import email.utils\n",
        "import enum\n",
        "import itertools\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import re\n",
        "import snscrape.base\n",
        "import string\n",
        "import time\n",
        "import typing\n",
        "import urllib.parse"
      ],
      "metadata": {
        "id": "SvMKI-2ER9oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56mG-0eTSDp1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}