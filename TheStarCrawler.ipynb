{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkWvcRk2LfmB"
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import threading\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyliinfire5j",
        "outputId": "010d86c8-c236-41ed-817f-31d72cf71581"
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import threading"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib) (0.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3BrRSO9Tthn"
      },
      "source": [
        "https://www.thestar.com.my/search?pgno=2&q=vaccine&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31\n",
        "\n",
        "https://www.thestar.com.my/search/?q=vaccine&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T2Uu9goLTKn"
      },
      "source": [
        "source = requests.get(\"https://www.thestar.com.my/search/?q=vaccine&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31\").text\n",
        "soup = BeautifulSoup(source, 'lxml')\n",
        "print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ymN-vyM0Re"
      },
      "source": [
        "topics = soup.find_all(class_=\"row list-listing\")\n",
        "\n",
        "csv_file = open('csv_scrape.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "\n",
        "for topic in topics:\n",
        "  headline = topic.div.h2.a.text\n",
        "  print(headline)\n",
        "  time_meta = topic.div.span.text\n",
        "  date_posted = time_meta.split('|')\n",
        "  date_posted = [item.strip() for item in date_posted]\n",
        "  reference = topic.div.h2.a.get('href')\n",
        "  print(date_posted)\n",
        "  print(reference)\n",
        "  csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "csv_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTD5L4EEUXow"
      },
      "source": [
        "csv_file = open('csv_scrape.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "target = 'vaccine'\n",
        "\n",
        "for page in range(335):\n",
        "  print(f'Crawling page {page}')\n",
        "  url = f'https://www.thestar.com.my/search?pgno={page}&q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "  source = requests.get(url).text\n",
        "  soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "  topics = soup.find_all(class_=\"row list-listing\")\n",
        "  for topic in topics:\n",
        "    headline = topic.div.h2.a.text\n",
        "    time_meta = topic.div.span.text\n",
        "    date_posted = time_meta.split('|')\n",
        "    date_posted = [item.strip() for item in date_posted]\n",
        "    reference = topic.div.h2.a.get('href')\n",
        "    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "  time.sleep(1)\n",
        "\n",
        "csv_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCy8PamEaIV7"
      },
      "source": [
        "def crawler_The_Star(targets):\n",
        "\n",
        "  if isinstance(targets, list):\n",
        "    pass\n",
        "  else:\n",
        "    targets = [targets]\n",
        "\n",
        "  for target in targets:\n",
        "    url = f'https://www.thestar.com.my/search?q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "    source = requests.get(url).text\n",
        "    soup = BeautifulSoup(source, 'lxml')\n",
        "    regex = r\"a*[0-9]+\"\n",
        "    page_navs = soup.find_all(id=\"slcontent_0_ThisPageRowCounts\")\n",
        "    for page_nav in page_navs:\n",
        "      page_list = page_nav.text\n",
        "      page_iter = re.finditer(regex, page_list)\n",
        "      for match in page_iter:\n",
        "        largest_query = match.group(0)\n",
        "    largest_page = math.ceil(float(largest_query)/30)\n",
        "    print(f'The total number of query result for the target word {target} is {largest_query}, with {largest_page} pages.')\n",
        "\n",
        "    csv_file = open(f'{target}.csv', 'w')\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "    for page in range(1, largest_page):\n",
        "      print(f'Crawling page {page} of target {target}')\n",
        "      url = f'https://www.thestar.com.my/search?pgno={page}&q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "      source = requests.get(url).text\n",
        "      soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "      topics = soup.find_all(class_=\"row list-listing\")\n",
        "      for topic in topics:\n",
        "        headline = topic.div.h2.a.text\n",
        "        time_meta = topic.div.span.text\n",
        "        date_posted = time_meta.split('|')\n",
        "        date_posted = [item.strip() for item in date_posted]\n",
        "        reference = topic.div.h2.a.get('href')\n",
        "        csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "      time.sleep(0.5)\n",
        "\n",
        "    csv_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8d4DoFvcTmE"
      },
      "source": [
        "crawler_The_Star('vaccine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMLvJbm5pUTi"
      },
      "source": [
        "content_url = 'https://www.thestar.com.my/opinion/letters/2020/01/13/engaging-with-anti-vaxxers'\n",
        "content_source = requests.get(content_url).text\n",
        "content_soup = BeautifulSoup(content_source, 'lxml')\n",
        "\n",
        "content_topics = content_soup.find_all('p')\n",
        "news_body = []\n",
        "##print(content_topics)\n",
        "\n",
        "for content_topic in content_topics:\n",
        "    content_body = content_topic.text\n",
        "    #print(content_body)\n",
        "    news_body.append(content_body)\n",
        "del news_body[-8:]\n",
        "news_content = \"\\n\".join(news_body).strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnHsYNgAjc89"
      },
      "source": [
        "class The_Star_Crawler:\n",
        "    def __init__(self, targets, sleeptime, verbose):\n",
        "        if isinstance(targets, list):\n",
        "            self.targets = targets\n",
        "        else:\n",
        "            self.targets = [targets]\n",
        "        self.sleeptime = sleeptime\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def obtainPageNumber(self, target):\n",
        "        url = f'https://www.thestar.com.my/search?q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "        source = requests.get(url).text\n",
        "        soup = BeautifulSoup(source, 'lxml')\n",
        "        regex = r\"a*[0-9]+\"\n",
        "        page_navs = soup.find_all(id=\"slcontent_0_ThisPageRowCounts\")\n",
        "        for page_nav in page_navs:\n",
        "            page_list = page_nav.text\n",
        "            page_iter = re.finditer(regex, page_list)\n",
        "            for match in page_iter:\n",
        "                largest_query = match.group(0)\n",
        "        largest_page = math.ceil(float(largest_query)/30)\n",
        "        print(f'The total number of query result for the target word {target} is {largest_query}, with {largest_page} pages.')\n",
        "\n",
        "        return largest_page\n",
        "\n",
        "    def crawl(self, largest_page, target, sleeptime):\n",
        "        csv_file = open(f'{target}.csv', 'w')\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        if (self.verbose):\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link', 'content'])\n",
        "        else:\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "        for page in range(1, largest_page):\n",
        "            print(f'Crawling page {page} of target {target}')\n",
        "            url = f'https://www.thestar.com.my/search?pgno={page}&q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "            source = requests.get(url).text\n",
        "            soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "            topics = soup.find_all(class_=\"row list-listing\")\n",
        "\n",
        "            for topic in topics:\n",
        "                headline = topic.div.h2.a.text\n",
        "                time_meta = topic.div.span.text\n",
        "                date_posted = time_meta.split('|')\n",
        "                date_posted = [item.strip() for item in date_posted]\n",
        "                reference = topic.div.h2.a.get('href')\n",
        "\n",
        "                if (self.verbose):\n",
        "                    content_url = reference\n",
        "                    content_source = requests.get(content_url).text\n",
        "                    content_soup = BeautifulSoup(content_source, 'lxml')\n",
        "\n",
        "                    content_topics = content_soup.find_all('p')\n",
        "                    news_body = []\n",
        "                    ##print(content_topics)\n",
        "\n",
        "                    for content_topic in content_topics:\n",
        "                        content_body = content_topic.text\n",
        "                        #print(content_body)\n",
        "                        news_body.append(content_body)\n",
        "                    del news_body[-8:]\n",
        "                    news_content = \"\\n\".join(news_body).strip()\n",
        "\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference, news_content])\n",
        "                else:\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "            time.sleep(sleeptime)\n",
        "\n",
        "        csv_file.close()\n",
        "\n",
        "    def run(self):\n",
        "        for target in self.targets:\n",
        "            pageNo = self.obtainPageNumber(target)\n",
        "            self.crawl(pageNo, target, self.sleeptime)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YWdioUmVoGeS"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    targets = ['antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'moderna', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking']\n",
        "    web_crawler = The_Star_Crawler(targets,0.1, False)\n",
        "    web_crawler.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdjtpHsLeAKk"
      },
      "source": [
        "https://www.freemalaysiatoday.com/page/1/?s=vaccine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KCK3lMDpkAS"
      },
      "source": [
        "!pip install fake_useragent\n",
        "from fake_useragent import UserAgent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSZwJkGJgOaw"
      },
      "source": [
        "url = f'https://www.freemalaysiatoday.com/?s=vaccine'\n",
        "ua = UserAgent()\n",
        "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
        "source = requests.get(url, headers = headers)\n",
        "print(source.status_code)\n",
        "print(source.json())\n",
        "soup = BeautifulSoup(source.text, 'lxml')\n",
        "page_navs = soup.find_all('a')\n",
        "print(page_navs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0FaHOWqdprk"
      },
      "source": [
        "class FMT_Crawler:\n",
        "    def __init__(self, targets, sleeptime):\n",
        "        if isinstance(targets, list):\n",
        "            self.targets = targets\n",
        "        else:\n",
        "            self.targets = [targets]\n",
        "        self.sleeptime = sleeptime\n",
        "\n",
        "    def obtainPageNumber(self, target):\n",
        "        url = f'https://www.freemalaysiatoday.com/?s={target}'\n",
        "        source = requests.get(url).text\n",
        "        soup = BeautifulSoup(source, 'lxml')\n",
        "        page_navs = soup.find_all(class_=\"last\")\n",
        "        print(page_navs)\n",
        "        for page_nav in page_navs:\n",
        "            page_list = page_nav.text\n",
        "            print(page_list)\n",
        "        largest_page = int(page_list)\n",
        "        print(f'The total number of query page for the target word {target} is {largest_page}.')\n",
        "\n",
        "        return largest_page\n",
        "\n",
        "    def crawl(self, largest_page, target, sleeptime):\n",
        "        csv_file = open(f'{target}.csv', 'w')\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['headline', 'date', 'time', 'link', 'content'])\n",
        "\n",
        "        for page in range(1, largest_page):\n",
        "            print(f'Crawling page {page} of target {target}')\n",
        "            url = f'https://www.freemalaysiatoday.com/page/{page}/?s={target}'\n",
        "            source = requests.get(url).text\n",
        "            soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "            topics = soup.find_all(class_=\"td_module_10 td_module_wrap td-animation-stack\")\n",
        "\n",
        "            for topic in topics:\n",
        "                headline = topic.div.h2.a.text\n",
        "                time_meta = topic.div.span.text\n",
        "                date_posted = time_meta.split('|')\n",
        "                date_posted = [item.strip() for item in date_posted]\n",
        "                reference = topic.div.h2.a.get('href')\n",
        "\n",
        "                content_url = reference\n",
        "                content_source = requests.get(content_url).text\n",
        "                content_soup = BeautifulSoup(content_source, 'lxml')\n",
        "\n",
        "                content_topics = content_soup.find_all('p')\n",
        "                news_body = []\n",
        "                ##print(content_topics)\n",
        "\n",
        "                for content_topic in content_topics:\n",
        "                    content_body = content_topic.text\n",
        "                    #print(content_body)\n",
        "                    news_body.append(content_body)\n",
        "                del news_body[-8:]\n",
        "                news_content = \"\\n\".join(news_body).strip()\n",
        "\n",
        "                csv_writer.writerow([headline, date_posted[0], date_posted[1], reference, news_content])\n",
        "\n",
        "            time.sleep(sleeptime)\n",
        "\n",
        "        csv_file.close()\n",
        "\n",
        "    def run(self):\n",
        "        for target in self.targets:\n",
        "            pageNo = self.obtainPageNumber(target)\n",
        "            self.crawl(pageNo, target, self.sleeptime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr52yKTueYDl"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    web_crawler = FMT_Crawler('vaccine',0.5)\n",
        "    web_crawler.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYx7wqpY-oyU"
      },
      "source": [
        "['ventilator', 'pcr', 'testing', 'antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXVA-qgmvvHA",
        "outputId": "c18e42e0-87c7-4cf4-d5dd-5fd593433ccf"
      },
      "source": [
        "response = requests.get('https://www.sinchew.com.my/coronavirus/')\n",
        "response.status_code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "403"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "powAibvIh0iN"
      },
      "source": [
        "def create_project_directory(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        print(f'Creating project {directory}')\n",
        "        os.makedirs(directory)\n",
        "\n",
        "def create_data_files(project_name, base_url):\n",
        "    queue = project_name + '/queue.txt'\n",
        "    crawled = project_name + '/crawled.txt'\n",
        "    if not os.path.isfile(queue):\n",
        "        write_file(queue, base_url)\n",
        "    if not os.path.isfile(crawled):\n",
        "        write_file(crawled, '')\n",
        "\n",
        "def write_file(path, data):\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(data)\n",
        "        f.close()\n",
        "\n",
        "# Add data onto an existing file\n",
        "def append_to_file(path, data):\n",
        "    with open(path, 'a') as file:\n",
        "        file.write(data + '\\n')\n",
        "\n",
        "# Delete the contents of a file\n",
        "def delete_file_contents(path):\n",
        "    with open(path, 'w'):\n",
        "        pass\n",
        "\n",
        "# Read a file and convert each line to set items\n",
        "def file_to_set(file_name):\n",
        "    results = set()\n",
        "    with open(file_name, 'rt') as f:\n",
        "        for line in f:\n",
        "            results.add(line.replace('\\n', ''))\n",
        "    return results\n",
        "\n",
        "\n",
        "# Iterate through a set, each item will be a line in a file\n",
        "def set_to_file(links, file_name):\n",
        "    with open( file_name,\"w\") as f:\n",
        "        for l in sorted(links):\n",
        "            f.write(l+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUx2-edPjGJb",
        "outputId": "9115b8c2-a282-4e15-b60c-df256e529ea1"
      },
      "source": [
        "import logging\n",
        "import threading\n",
        "import time\n",
        "\n",
        "def thread_function(name):\n",
        "    logging.info(\"Thread %s: starting\", name)\n",
        "    time.sleep(2)\n",
        "    logging.info(\"Thread %s: finishing\", name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    format = \"%(asctime)s: %(message)s\"\n",
        "    logging.basicConfig(format=format, level=logging.INFO,\n",
        "                        datefmt=\"%H:%M:%S\")\n",
        "\n",
        "    logging.info(\"Main    : before creating thread\")\n",
        "    x = threading.Thread(target=thread_function, args=(1,))\n",
        "    x.start()\n",
        "    x = threading.Thread(target=thread_function, args=(2,))\n",
        "    x.start()\n",
        "    logging.info(\"Main    : before running thread\")\n",
        "    logging.info(\"Main    : wait for the thread to finish\")\n",
        "    # x.join()\n",
        "    logging.info(\"Main    : all done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "00:58:48: Main    : before creating thread\n",
            "00:58:48: Thread 1: starting\n",
            "00:58:48: Thread 2: starting\n",
            "00:58:48: Main    : before running thread\n",
            "00:58:48: Main    : wait for the thread to finish\n",
            "00:58:48: Main    : all done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pdLaa5pRil"
      },
      "source": [
        "class The_Star_Crawler:\n",
        "    def __init__(self, targets, sleeptime, directory, verbose):\n",
        "        if isinstance(targets, list):\n",
        "            self.targets = targets\n",
        "        else:\n",
        "            self.targets = [targets]\n",
        "        self.sleeptime = sleeptime\n",
        "        self.verbose = verbose\n",
        "        self.directory = directory\n",
        "\n",
        "    def obtainPageNumber(self, target):\n",
        "        url = f'https://www.thestar.com.my/search?q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "        source = requests.get(url).text\n",
        "        soup = BeautifulSoup(source, 'lxml')\n",
        "        regex = r\"a*[0-9]+\"\n",
        "        page_navs = soup.find_all(id=\"slcontent_0_ThisPageRowCounts\")\n",
        "        for page_nav in page_navs:\n",
        "            page_list = page_nav.text\n",
        "            page_iter = re.finditer(regex, page_list)\n",
        "            for match in page_iter:\n",
        "                largest_query = match.group(0)\n",
        "        largest_page = math.ceil(float(largest_query)/30)\n",
        "        print(f'The total number of query result for the target word {target} is {largest_query}, with {largest_page} pages.')\n",
        "\n",
        "        return largest_page\n",
        "\n",
        "    def create_project(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'Creating project {directory}')\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    def crawl(self, largest_page, target, sleeptime):\n",
        "        csv_file = open(f'{self.directory}/{target}.csv', 'w')\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        if (self.verbose):\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link', 'content'])\n",
        "        else:\n",
        "            csv_writer.writerow(['headline', 'date', 'time', 'link'])\n",
        "\n",
        "        for page in range(1, largest_page):\n",
        "            print(f'Crawling page {page} of target {target}')\n",
        "            url = f'https://www.thestar.com.my/search?pgno={page}&q={target}&qguid=&qtag=&QDR=QDR_specific&qsort=oldest&qrec=30&sdate=2019-12-01&edate=2021-12-31'\n",
        "            time.sleep(0.015)\n",
        "            source = requests.get(url).text\n",
        "            soup = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "            topics = soup.find_all(class_=\"row list-listing\")\n",
        "\n",
        "            for topic in topics:\n",
        "                headline = topic.div.h2.a.text\n",
        "                time_meta = topic.div.span.text\n",
        "                date_posted = time_meta.split('|')\n",
        "                date_posted = [item.strip() for item in date_posted]\n",
        "                reference = topic.div.h2.a.get('href')\n",
        "\n",
        "                if (self.verbose):\n",
        "                    content_url = reference\n",
        "                    content_source = requests.get(content_url).text\n",
        "                    content_soup = BeautifulSoup(content_source, 'lxml')\n",
        "\n",
        "                    content_topics = content_soup.find_all('p')\n",
        "                    news_body = []\n",
        "                    ##print(content_topics)\n",
        "\n",
        "                    for content_topic in content_topics:\n",
        "                        content_body = content_topic.text\n",
        "                        #print(content_body)\n",
        "                        news_body.append(content_body)\n",
        "                    del news_body[-8:]\n",
        "                    news_content = \"\\n\".join(news_body).strip()\n",
        "\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference, news_content])\n",
        "                else:\n",
        "                    csv_writer.writerow([headline, date_posted[0], date_posted[1], reference])\n",
        "\n",
        "            time.sleep(sleeptime)\n",
        "\n",
        "        csv_file.close()\n",
        "\n",
        "\n",
        "    def run(self, target):\n",
        "        pageNo = self.obtainPageNumber(target)\n",
        "        self.crawl(pageNo, target, self.sleeptime)\n",
        "\n",
        "    def multirun(self):\n",
        "        self.create_project(self.directory)\n",
        "\n",
        "        for index, target in enumerate(self.targets):\n",
        "            logging.info(\"Main    : create and start thread %d.\", index)\n",
        "            t = threading.Thread(target=self.run, args = (target, ))\n",
        "            t.start()\n",
        "        t.join()\n",
        "\n",
        "        !zip -r f'/content/{directory}{('Summary','')[verbose]}.zip /content/{directory}'\n",
        "        from google.colab import files\n",
        "        files.download(f\"/content/{directory}{('Summary','')[verbose]}.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTGjKVo9pV1V"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    targets = ['virus', 'ventilator', 'pcr', 'testing', 'antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking', 'fines']\n",
        "    web_crawler = The_Star_Crawler(targets, 0.1, 'TheStar', False)\n",
        "    web_crawler.multirun()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUgbmqGEckKU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}