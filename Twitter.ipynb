{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObPs_5A8ylA1"
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import string\n",
        "import random\n",
        "import datetime\n",
        "import threading\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
        "from time import sleep\n",
        "import json\n",
        "import datetime\n",
        "import tweepy\n",
        "import json\n",
        "import math\n",
        "import glob\n",
        "import csv\n",
        "import zipfile\n",
        "import zlib\n",
        "from tweepy import TweepError\n",
        "from time import sleep\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
        "from time import sleep\n",
        "import json\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2z3CAi5PaoM"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import string\n",
        "import random\n",
        "import datetime\n",
        "import threading\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
        "from time import sleep\n",
        "import json\n",
        "import datetime\n",
        "import tweepy\n",
        "import json\n",
        "import math\n",
        "import glob\n",
        "import csv\n",
        "import zipfile\n",
        "import zlib\n",
        "from tweepy import TweepError\n",
        "from time import sleep\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
        "from time import sleep\n",
        "import json\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import json\n",
        "\n",
        "\"\"\"chrome_options = webdriver.ChromeOptions()\n",
        "settings = {\n",
        "    \"appState\": {\n",
        "        \"recentDestinations\": [{\n",
        "            \"id\": \"Save as PDF\",\n",
        "            \"origin\": \"local\",\n",
        "            \"account\": \"\",\n",
        "        }],\n",
        "        \"selectedDestinationId\": \"Save as PDF\",\n",
        "        \"version\": 2\n",
        "    }\n",
        "}\n",
        "prefs = {'printing.print_preview_sticky_settings': json.dumps(settings)}\n",
        "chrome_options.add_experimental_option('prefs', prefs)\n",
        "chrome_options.add_argument('--kiosk-printing')\n",
        "CHROMEDRIVER_PATH = '/usr/local/bin/chromedriver'\n",
        "driver = webdriver.Chrome(chrome_options=chrome_options, executable_path=CHROMEDRIVER_PATH)\"\"\"\n",
        "wd.get(\"https://google.com\")\n",
        "wd.execute_script('window.print();')\n"
      ],
      "metadata": {
        "id": "giIp86LF8p4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riGErRiyQOfD"
      },
      "source": [
        "consumer_key= 'PFntvVTEGYxOj0DpsQGLfcHih'\n",
        "consumer_secret= 'qaMWaTihgx7WKCxs2CWf6VkJrGQMaQyBkd22KW6vhdHpRmO7yg'\n",
        "access_token= '994786472900333570-6qqX2SH8XbHXAbJRkWdwzge4A7xktPo'\n",
        "access_token_secret= 'ZryyU4n78wJyUQaNVDPNR7X7a7XKDAOtxy768SCMH2Y2E'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVinlbNnQxBm"
      },
      "source": [
        "userID = 'Khairykj'\n",
        "tweets = api.user_timeline(screen_name=userID,\n",
        "                           # 200 is the maximum allowed count\n",
        "                           count=5,\n",
        "                           include_rts = False,\n",
        "                           # Necessary to keep full_text\n",
        "                           # otherwise only the first 140 words are extracted\n",
        "                           tweet_mode = 'extended'\n",
        "                           )\n",
        "for info in tweets[:3]:\n",
        "     print(\"ID: {}\".format(info.id))\n",
        "     print(info.created_at)\n",
        "     print(info.full_text)\n",
        "     print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU9-Lrp-Rtmt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIwKBgnUTF2S"
      },
      "source": [
        "class TwitterInformation:\n",
        "\n",
        "    def __init__(self, userID, sleepTime, verbose):\n",
        "        self.userID = userID\n",
        "        self.tweets = []\n",
        "        self.oldest_id = 0\n",
        "        self.sleepTime = sleepTime\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def initTweet(self, userID):\n",
        "        self.tweets = api.user_timeline(screen_name = userID,\n",
        "                           # 200 is the maximum allowed count\n",
        "                           count = 5,\n",
        "                           include_rts = True,\n",
        "                           # Necessary to keep full_text\n",
        "                           # otherwise only the first 140 words are extracted\n",
        "                           tweet_mode = 'extended'\n",
        "                           )\n",
        "        print(f'now extracting {userID}')\n",
        "    def allTweets(self, userID):\n",
        "        try:\n",
        "            self.oldest_id = self.tweets[-1].id\n",
        "        except:\n",
        "            print(\"No Tweets from this account %s\" % userID)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                extract_tweets = api.user_timeline(screen_name=userID,\n",
        "                                    # 200 is the maximum allowed count\n",
        "                                    count=200,\n",
        "                                    include_rts = True,\n",
        "                                    max_id = self.oldest_id - 1,\n",
        "                                    # Necessary to keep full_text\n",
        "                                    # otherwise only the first 140 words are extracted\n",
        "                                    tweet_mode = 'extended'\n",
        "                                    )\n",
        "\n",
        "                if len(extract_tweets) == 0:\n",
        "                    break\n",
        "\n",
        "                self.oldest_id = extract_tweets[-1].id\n",
        "                self.tweets.extend(extract_tweets)\n",
        "                print('{} tweets has been downloaded'.format(len(self.tweets)))\n",
        "            except tw.TweepError:\n",
        "                print('Protected Tweets')\n",
        "\n",
        "\n",
        "    def write_file(self, tweets):\n",
        "        outtweets = [[tweet.id_str,\n",
        "                      tweet.created_at,\n",
        "                      str(f'https:/twitter.com/{userID}/status/{tweet.id_str}'),\n",
        "                      tweet.favorite_count,\n",
        "                      tweet.retweet_count,\n",
        "                      tweet.full_text.encode(\"utf-8\").decode(\"utf-8\")]\n",
        "                     for idx,tweet in enumerate(self.tweets)]\n",
        "        dt = {'id':str, \"created_at\": str, \"link\": str, \"favorite_count\": str, \"retweet_count\":str, \"text\":str }\n",
        "        df = pd.DataFrame(outtweets,columns=[\"id\",\"created_at\",\"link\",\"favorite_count\",\"retweet_count\", \"text\"])\n",
        "        df.astype(dt)\n",
        "        df.to_csv(f'{userID}_tweets.csv', index=True)\n",
        "\n",
        "    def obtainScreenshots(self, userID, sleepTime):\n",
        "        base_url = f'https:/twitter.com/{userID}/status'\n",
        "        for idx, tweet in enumerate(self.tweets):\n",
        "            id_tweet = tweet.id\n",
        "            url = f'{base_url}/{id_tweet}'\n",
        "            print(url)\n",
        "            self.takeScreenshots(url, userID, id_tweet, sleepTime)\n",
        "\n",
        "    def takeScreenshots(self, url, userID, id_tweet, sleepTime):\n",
        "        wd.get(url)\n",
        "        time.sleep(sleepTime)\n",
        "        wd.save_screenshot(f'/content/savedscreenshot/{userID}-{id_tweet}.png')\n",
        "\n",
        "    def create_dir(self):\n",
        "        if not os.path.exists('savedscreenshot'):\n",
        "            print(f'Creating project savedscreenshot')\n",
        "            os.makedirs('savedscreenshot')\n",
        "\n",
        "    def run(self):\n",
        "        try:\n",
        "            self.initTweet(self.userID)\n",
        "            self.allTweets(self.userID)\n",
        "            self.write_file(self.tweets)\n",
        "            if (self.verbose):\n",
        "                self.create_dir()\n",
        "                self.obtainScreenshots(self.userID, self.sleepTime)\n",
        "        except tw.TweepError:\n",
        "            print(\"Protected Account\")\n",
        "            pass\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qlJWVdmXDpG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b90a6761-045e-436e-f40e-0eae725114a4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    userIDs = ['drmuss', 'DrIqbalSarwar','khairul_hafidz', 'rafidah72 ','mafeitz','MYJantung','DrMahyuddin','drafidahyusof','suhazeli','khalilpsych','ctsarahaishah',\n",
        "               'afidasohana','muhammadeizat','zainaladwin','kay_alia','Lael_Ophthal','drihsansuhaidi','DrAnwarFazal','fdcaraku3800','syariza81','aimirahayu','affick_rahim',\n",
        "               'Zafrul_Zamberi','DrRadzi Surgery','wanie_dagreat','fiezanizmohamed','hafsah_sazali','FirdausIzzani','RahmatPsych','khiddir89','ib_shah','qonberg',\n",
        "               'azamspark','kay_haliq','AmiraRaup','wahidatul_abdah','marmellow','izzahrph','jazlinasyahrul','SmSmaudio','MarAzlan']\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "MKNJPM, MyHEALTHKKM, kkmm_gov, jpmgov_, KDNPUTRAJAYA,\n",
        "JKJAVMY, DrAdhamBaba, MuhyiddinYassin, DGHisham, IsmailSabri60\n",
        "RTM_Malaysia, AnnuarMusa, 501Awani, bharianmy,\n",
        "hannahyeoh, rafidah72, DrAnwarFazal, MedTweetMYHQ, khairul_hafidz, BuletinTV3\n",
        "ChristinaYong6, loyingru, SarehParangiMD, radzi_dr, ClinicalRsrchMY,\n",
        "Khairykj, SandboxMalaysia, sputnikvaccine, NIHMalaysia, protecthealthco,\n",
        "trussliz, SelveeRamasamy, DrMahHS, malaysianmedic1, angepratt,\n",
        "takeshi_kasai, codebluenews, imokman, DrDzul, HarithIskander,\n",
        "UMonline, fmtoday, SinarOnline, mkini_bm,\n",
        "\"\"\"\n",
        "    userIDs = ['DrAdhamBaba', 'MuhyiddinYassin', 'AnnuarMusa', 'ChristinaYong6', 'SandboxMalaysia', 'protecthealthco']\n",
        "    for userID in userIDs:\n",
        "        tw_evidence = TwitterInformation(userID, 1, False)\n",
        "        tw_evidence.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now extracting DrAdhamBaba\n",
            "205 tweets has been downloaded\n",
            "405 tweets has been downloaded\n",
            "605 tweets has been downloaded\n",
            "805 tweets has been downloaded\n",
            "1005 tweets has been downloaded\n",
            "1205 tweets has been downloaded\n",
            "1402 tweets has been downloaded\n",
            "1602 tweets has been downloaded\n",
            "1802 tweets has been downloaded\n",
            "2002 tweets has been downloaded\n",
            "2201 tweets has been downloaded\n",
            "2401 tweets has been downloaded\n",
            "2601 tweets has been downloaded\n",
            "2801 tweets has been downloaded\n",
            "3001 tweets has been downloaded\n",
            "3201 tweets has been downloaded\n",
            "3245 tweets has been downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-18-9cf8e3041868>\", line 22, in <module>\n",
            "    tw_evidence.run()\n",
            "  File \"<ipython-input-17-af7f83099fc2>\", line 83, in run\n",
            "    self.write_file(self.tweets)\n",
            "  File \"<ipython-input-17-af7f83099fc2>\", line 59, in write_file\n",
            "    df.to_csv(f'{userID}_tweets.csv', index=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\", line 3170, in to_csv\n",
            "    formatter.save()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\", line 190, in save\n",
            "    compression=dict(self.compression_args, method=self.compression),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\", line 493, in get_handle\n",
            "    f = open(path_or_buf, mode, encoding=encoding, errors=errors, newline=\"\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'DrAdhamBaba_tweets.csv'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxV7X4dB6HRa"
      },
      "source": [
        "!zip -r /content/KDNPUTRAJAYA.zip /content\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_ICOnjg76pfS",
        "outputId": "1993edf9-1f40-4957-8a85-83cc64801029"
      },
      "source": [
        "files.download(f\"/content/MedTweetMy.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ccc92d41-2f53-41dd-97bc-e36bb68b466b\", \"MedTweetMy.zip\", 14814469)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRRIR2G17PtP",
        "outputId": "0c189fca-d414-4329-e42f-115c12e42e6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R888vh2A7syF",
        "outputId": "7a54783f-5aef-4cdc-8f18-e394710dcbe7"
      },
      "source": [
        "!cp -r /content/KhairyKjTweet.zip /content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: target 'Drive/' is not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv924lO756Bd"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aXCMtgEeP7h",
        "outputId": "1f53c115-0e34-4af4-e060-0e720eff29f6"
      },
      "source": [
        "url = 'https:/twitter.com/Khairykj/status/1458688001052057603'\n",
        "path = '/path/to/save/in/scrape.png'\n",
        "\n",
        "\n",
        "wd.get(url)\n",
        "el = wd.find_element_by_tag_name('body')\n",
        "time.sleep(1)\n",
        "el.screenshot('testing.png')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei7r8pNCxHXK"
      },
      "source": [
        "wd.get(\"https://twitter.com/TellYourSonThis/status/1455967631442329608\")\n",
        "time.sleep(1)\n",
        "wd.save_screenshot('ss1.png')\n",
        "screenshot = Image.open('ss1.png')\n",
        "screenshot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQwqmWvbxcKy"
      },
      "source": [
        "def twitter_link_processing(file_name, save_path_name, webdriver):\n",
        "  regex = r\"a*[0-9]+\"\n",
        "  with open(file_name, 'r') as f:\n",
        "    for row in f:\n",
        "      print(row)\n",
        "      screenshot_name_iter = re.finditer(regex, row)\n",
        "      for match in screenshot_name_iter:\n",
        "        screenshot_name = match.group(0)\n",
        "      webdriver.get(row)\n",
        "      time.sleep(1)\n",
        "      wd.save_screenshot(save_path_name + '/' + screenshot_name + '.png')\n",
        "      print(screenshot_name + '.txt is saved in the folder ' + save_path_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOan2_f0IAn"
      },
      "source": [
        "twitter_link_processing('KJ3.txt', 'Trial', wd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OywbPJ3_vb5m",
        "outputId": "76989cef-d0d2-46d2-e427-4330b6761868"
      },
      "source": [
        "url = f'https://www.malaymail.com/news/malaysia/2019/12?page=33'\n",
        "source = requests.get(url)\n",
        "print(source.status_code)\n",
        "soup = BeautifulSoup(source.text, 'lxml')\n",
        "reference = soup.find_all('a', {'class':'py-2 d-block'})\n",
        "if reference:\n",
        "    print(1)\n",
        "hyperlink = [link.get('href') for link in reference]\n",
        "title = [link.text for link in reference]\n",
        "print(title)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAuttSm8xJes"
      },
      "source": [
        "class MalayMailScraper:\n",
        "    base_url = f'https://www.malaymail.com/'\n",
        "    relevancy = ['pcr', 'testing', 'antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking']\n",
        "    #scraped results\n",
        "    results = []\n",
        "\n",
        "    def fetch(self, type, year, month, page):\n",
        "        # Make HTTP GET request\n",
        "        url = f'{self.base_url}news/{type}/{year}/{month}?page={page}'\n",
        "        print(url)\n",
        "        response = requests.get(url)\n",
        "        #print('type: %s, HTTP GET request to URL: %s | Status code: %s' % (type, response.url, response.status_code))\n",
        "\n",
        "        # Return HTTP response\n",
        "        time.sleep(0.5)\n",
        "        return response\n",
        "\n",
        "    def store_response(self, response):\n",
        "        '''Stores HTML response to file for debugging parser'''\n",
        "        # If response is OK\n",
        "        if response.status_code == 200:\n",
        "            print('Saving response to \"res.html\"... ', end='')\n",
        "            # Write response to HTML file\n",
        "            with open('res.html', 'w') as html_file:\n",
        "                html_file.write(response.text)\n",
        "            print('Done')\n",
        "        else:\n",
        "            print('Bad response!')\n",
        "\n",
        "    def load_response(self):\n",
        "        '''Loads HTML response for debugging parser'''\n",
        "        html = ''\n",
        "        # Open HTML file\n",
        "        with open('res.html', 'r') as html_file:\n",
        "            for line in html_file.read():\n",
        "                html += line\n",
        "\n",
        "        # Return HTML as string\n",
        "        return html\n",
        "\n",
        "    def relevancy_check(self, hyperlink_lst, title_lst):\n",
        "        hyperlinks = [link for link in hyperlink_lst]\n",
        "        titles = [title.lower() for title in title_lst]\n",
        "        delno = 0\n",
        "\n",
        "        #check for relevancy\n",
        "        for index in range(len(titles)):\n",
        "            tokens = title_lst[index].split()\n",
        "            relevant = False\n",
        "            for check in self.relevancy:\n",
        "                if (check in tokens):\n",
        "                    relevant = True\n",
        "                    break\n",
        "            if not relevant:\n",
        "                del titles[index-delno]\n",
        "                del hyperlinks[index-delno]\n",
        "                delno = delno + 1\n",
        "            relevant = False\n",
        "\n",
        "        return {\n",
        "            'links': hyperlinks,\n",
        "            'titles': titles,\n",
        "            'queries': len(titles) - delno\n",
        "        }\n",
        "\n",
        "    def parse(self, html, year, month):\n",
        "        '''Parses response's text and extract data from it'''\n",
        "\n",
        "        # Parse content\n",
        "        content = BeautifulSoup(html, 'lxml')\n",
        "        # Extract data\n",
        "        reference = content.find_all('a', {'class':'py-2 d-block'})\n",
        "        if reference:\n",
        "            hyperlink = [link.get('href') for link in reference]\n",
        "            title = [link.text.lower() for link in reference]\n",
        "\n",
        "            relevancy_dict = self.relevancy_check(hyperlink, title)\n",
        "            title_copied = relevancy_dict['titles']\n",
        "            hyperlink_copied = relevancy_dict['links']\n",
        "\n",
        "            # Loop over the number of entries\n",
        "            for index in range(0, len(title_copied)):\n",
        "                # Append extracted data to results list\n",
        "                self.results.append({\n",
        "                    'title': title_copied[index],\n",
        "                    'year': year,\n",
        "                    'month': month,\n",
        "                    'link': hyperlink_copied[index]\n",
        "                })\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def write_csv(self, csv_filename):\n",
        "        '''Writes scpared results to CSV file'''\n",
        "\n",
        "        # Check results list in not empty\n",
        "        if len(self.results):\n",
        "            print(f'Writing results to {csv_filename}')\n",
        "\n",
        "            # Open file stream to write CSV\n",
        "            with open(csv_filename, 'w') as csv_file:\n",
        "                # Init CSV dictionary writer\n",
        "                writer = csv.DictWriter(csv_file, fieldnames=self.results[0].keys())\n",
        "\n",
        "                # Write column names to file\n",
        "                writer.writeheader()\n",
        "\n",
        "                # Write results list to CSV file\n",
        "                for row in self.results:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "            print('Done')\n",
        "\n",
        "    def run(self):\n",
        "        '''Starts crawler'''\n",
        "        types = ['malaysia', 'world']\n",
        "        years = ['2019', '2020', '2021']\n",
        "        types = ['malaysia', 'world']\n",
        "        months = ['01', '02' ,'03','04','05','06','07','08','09','10','11','12']\n",
        "        # Loop over the range of pages to scrape\n",
        "        for type in types:\n",
        "            for year in years:\n",
        "                for month in months:\n",
        "                    if (year == '2019'):\n",
        "                            continue\n",
        "\n",
        "                    content_exist = True\n",
        "                    page = 1\n",
        "                    while content_exist:\n",
        "                        # Make HTTP GET request\n",
        "                        response = self.fetch(type, year, month, page)\n",
        "                        # Parse content\n",
        "                        content_exist = self.parse(response.text, year, month)\n",
        "                        page = page + 1\n",
        "\n",
        "        # Wait for 2 sec\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Write scraped results to CSV file\n",
        "        self.write_csv('malaymail_summary.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV1ddYc_j9Ht"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    web_crawler = MalayMailScraper()\n",
        "    web_crawler.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ZH1vn9rEar"
      },
      "source": [
        "!pip install pyTelegramBotAPI\n",
        "\n",
        "import telebot\n",
        "import os\n",
        "\n",
        "API_KEY = os.getenv('2136512154:AAELzyC9frqvtgQWWBOJw9q0f2bAyFndzxQ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8iV02zIrCuh"
      },
      "source": [
        "class MalayMailScraper:\n",
        "\n",
        "    def __init__(self, base_url, relevancy, slept, sleeptime_mean, sleeptime_var, directory, verbose):\n",
        "        self.base_url = base_url\n",
        "        self.relevancy = relevancy\n",
        "        self.slept = slept\n",
        "        self.sleeptime_mean = sleeptime_mean\n",
        "        self.sleeptime_var = sleeptime_var\n",
        "        self.directory = directory\n",
        "        self.verbose = verbose\n",
        "        if (self.verbose):\n",
        "            self.results = pd.DataFrame(data = None, columns = ['Title', 'Year', 'Month', 'Link', 'Content'])\n",
        "        else:\n",
        "            self.results = pd.DataFrame(data = None, columns = ['Title', 'Year', 'Month', 'Link'])\n",
        "\n",
        "    def create_project(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'Creating project {directory}')\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    def fetch(self, base_url, type, year, month, page, slept, sleeptime_mean, sleeptime_var):\n",
        "        # Make HTTP GET request\n",
        "        url = f'{base_url}news/{type}/{year}/{month}?page={page}'\n",
        "        response = requests.get(url)\n",
        "        print('type: %s, HTTP GET request to URL: %s | Status code: %s' % (type, response.url, response.status_code))\n",
        "\n",
        "        # Return HTTP response\n",
        "        if(slept):\n",
        "            time.sleep(sleeptime_mean + random.random()*sleeptime_var)\n",
        "        return response\n",
        "\n",
        "    def subfetchParse(self, url, slept, sleeptime_mean, sleeptime_var):\n",
        "        corpus = ''\n",
        "        if(slept):\n",
        "            time.sleep(sleeptime_mean/10 + random.random()*sleeptime_var/10)\n",
        "        response = requests.get(url)\n",
        "        content = BeautifulSoup(response.text, 'lxml')\n",
        "        article_data = content.find_all('p')\n",
        "        corpus = []\n",
        "        if article_data:\n",
        "            for article in article_data:\n",
        "                paras = article.text\n",
        "                corpus.append(paras)\n",
        "        corpus = ' '.join(corpus)\n",
        "        return corpus\n",
        "\n",
        "    def parse(self, html, year, month, relevancy,  slept, sleeptime_mean, sleeptime_var, verbose):\n",
        "        '''Parses response's text and extract data from it'''\n",
        "\n",
        "        # Parse content\n",
        "        content = BeautifulSoup(html, 'lxml')\n",
        "        # Extract data\n",
        "        reference = content.find_all('a', {'class':'py-2 d-block'})\n",
        "        if reference:\n",
        "            hyperlink = [link.get('href') for link in reference]\n",
        "            title = [link.text.lower() for link in reference]\n",
        "\n",
        "            relevancy_dict = self.relevancy_check(relevancy, hyperlink, title)\n",
        "            title_copied = relevancy_dict['titles']\n",
        "            hyperlink_copied = relevancy_dict['links']\n",
        "\n",
        "            if (verbose):\n",
        "                # Loop over the number of entries\n",
        "                for index in range(0, len(title_copied)):\n",
        "                    # Append extracted data to results list\n",
        "                    self.results = self.results.append({\n",
        "                        'Title': title_copied[index],\n",
        "                        'Year': year,\n",
        "                        'Month': month,\n",
        "                        'Link': hyperlink_copied[index]\n",
        "                    }, ignore_index=True)\n",
        "            else:\n",
        "                for index in range(0, len(title_copied)):\n",
        "                    # Append extracted data to results list\n",
        "                    corpus = self.subfetchParse(hyperlink_copied[index], slept, sleeptime_mean, sleeptime_var)\n",
        "                    data = {\n",
        "                        'Title': title_copied[index],\n",
        "                        'Year': year,\n",
        "                        'Month': month,\n",
        "                        'Link': hyperlink_copied[index],\n",
        "                        'Content': corpus\n",
        "                    }\n",
        "                    self.results = self.results.append(data, ignore_index=True)\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def relevancy_check(self, relevancy, hyperlink_lst, title_lst):\n",
        "        hyperlinks = [link for link in hyperlink_lst]\n",
        "        titles = [title.lower() for title in title_lst]\n",
        "        delno = 0\n",
        "\n",
        "        #check for relevancy\n",
        "        for index in range(len(titles)):\n",
        "            tokens = title_lst[index].split()\n",
        "            relevant = False\n",
        "            for check in relevancy:\n",
        "                if (check in tokens):\n",
        "                    relevant = True\n",
        "                    break\n",
        "            if not relevant:\n",
        "                del titles[index-delno]\n",
        "                del hyperlinks[index-delno]\n",
        "                delno = delno + 1\n",
        "            relevant = False\n",
        "\n",
        "        return {\n",
        "            'links': hyperlinks,\n",
        "            'titles': titles,\n",
        "            'queries': len(titles) - delno\n",
        "        }\n",
        "\n",
        "    def write_file(self, directory, excel_filename):\n",
        "        '''Writes scpared results to CSV file'''\n",
        "\n",
        "        # Check results list in not empty\n",
        "        if len(self.results):\n",
        "            print(f'Writing results to {directory}/{excel_filename}')\n",
        "\n",
        "            self.results.to_excel(f'/content/{directory}/{excel_filename}')\n",
        "\n",
        "            print('Done')\n",
        "\n",
        "    def save_file(self, directory, excel_filename, verbose):\n",
        "        !zip -r /content/MalayMail.zip /content/MalayMail\n",
        "        from google.colab import files\n",
        "        files.download(f\"/content/MalayMail.zip\")\n",
        "\n",
        "    def multirun(self):\n",
        "        types = ['malaysia', 'world']\n",
        "        years = ['2020', '2021']\n",
        "        months = ['01', '02' ,'03','04','05','06','07','08','09','10','11','12']\n",
        "        threads = []\n",
        "        self.create_project(self.directory)\n",
        "        # Loop over the range of pages to scrape\n",
        "        for type in types:\n",
        "            for year in years:\n",
        "                for month in months:\n",
        "                    t = threading.Thread(target=self.run, args = (self.base_url, type, year, month,\n",
        "                                                                  self.slept, self.sleeptime_mean, self.sleeptime_var,\n",
        "                                                                  self.verbose,))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "        for thr in threads:\n",
        "            thr.join()\n",
        "\n",
        "\n",
        "        self.write_file(self.directory, 'MalayMail.xlsx')\n",
        "        self.save_file (self.directory, 'MalayMail.xlsx', self.verbose)\n",
        "\n",
        "    def run(self, base_url, type, year, month, slept, sleeptime_mean, sleeptime_var, verbose):\n",
        "        content_exist = True\n",
        "        page = 1\n",
        "        while content_exist:\n",
        "            # Make HTTP GET request\n",
        "\n",
        "            response = self.fetch(base_url, type, year, month, page, slept, sleeptime_mean, sleeptime_var)\n",
        "            # Parse content\n",
        "            content_exist = self.parse(response.text, year, month, relevancy,\n",
        "                                       slept, sleeptime_mean, sleeptime_var, verbose)\n",
        "            page = page + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMdfpHHECvdH"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    base_url = 'https://www.malaymail.com/'\n",
        "    relevancy = ['paxlovid', 'molnupiravir', 'merck', 'sars-cov-2', 'pcr', 'virus', 'ventilator', 'mysejahtera', 'anti-vaxxers', 'antigen', 'misinformation', 'covidnow', 'deaths', 'dg', 'noor', 'hisham', 'vaccinated', 'unvaccinated', 'vaccination', 'anti-vaxxers', 'vaccine hesitancy', 'mandate', 'pfizer', 'moderna', 'sinovac', 'az', 'astrazeneca', 'sinopharm', 'mrna', 'ivermectin', 'hydroxychloroquine', 'khairy', 'mco', 'emco', 'cmco', 'rmco', 'covid-19', 'moh', 'distancing', 'mask', 'masking']\n",
        "    random.seed(datetime.datetime.now())\n",
        "    web_crawler = MalayMailScraper(base_url, relevancy, True, 0.5, 0.3, 'MalayMail', True)\n",
        "    web_crawler.multirun()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_2XrUMeVwo6"
      },
      "source": [
        "web_crawler.results.to_excel('/content/MalayMail/MalayMail.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-M1jLhqkE6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitaTCqOkGCT"
      },
      "source": [
        "class BernamaScraper:\n",
        "    def __init__(self, base_url, language, directory, verbose):\n",
        "        self.base_url = base_url\n",
        "        self.verbose = verbose\n",
        "        self.language = language\n",
        "        self.directory = directory\n",
        "        self.content = pd.DataFrame(data = None, columns = ['title', 'date', 'link', 'corpus'])\n",
        "    \"\"\"\n",
        "    content = {\n",
        "        'title':\n",
        "        'date':\n",
        "        'link':\n",
        "        'corpus':\n",
        "    }\n",
        "    base_url = 'https://www.bernama.com'\n",
        "    language = 'defLanguage(language)'\n",
        "    meta = 'covid-19.php?page={num}'\n",
        "    \"\"\"\n",
        "\n",
        "    def create_project(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'Creating project {directory}')\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    def defLanguage(self, language):\n",
        "        if (language == 'en'):\n",
        "            return 'en/general'\n",
        "        elif (language == 'bm'):\n",
        "            return 'bm/am'\n",
        "        else:\n",
        "            return 'en/general'\n",
        "\n",
        "    def fetch(self, base_url, language, pageNum, id):\n",
        "        la = self.defLanguage(language)\n",
        "\n",
        "        if not (id):\n",
        "            url = f'{base_url}/{la}/covid-19.php?page={pageNum}'\n",
        "            time.sleep(0.015)\n",
        "            response = requests.get(url)\n",
        "            print('HTTP GET request to URL: %s | Status code: %s' % (response.url, response.status_code))\n",
        "            self.sleep(0.5, 0.2)\n",
        "            return response.text\n",
        "        else:\n",
        "            url = f'{base_url}/{la}/{id}'\n",
        "            time.sleep(0.015)\n",
        "            response = requests.get(url)\n",
        "            print('HTTP GET request to URL: %s | Status code: %s' % (response.url, response.status_code))\n",
        "            self.sleep(0.5, 0.2)\n",
        "            return response.text\n",
        "\n",
        "    def obtainLargestPageNum(self, html):\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        reference = soup.find_all('a',{'class':\"page-link\", \"line-height\": \"17px\"})\n",
        "        pageNum = None\n",
        "\n",
        "        if reference:\n",
        "            pageNum = [int(num.text) for num in reference]\n",
        "            return max(pageNum)\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def parseList(self, html):\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        reference = soup.find_all('a',{'class':\"text-dark text-decoration-none\"})\n",
        "        if reference:\n",
        "            substr = 'news_covid-19'\n",
        "            hyperlinkID = [link.get('href') for link in reference if substr in link.get('href') ]\n",
        "            title = [link.text.lower() for link in reference if substr in link.get('href')]\n",
        "            data = []\n",
        "            for idx, t in enumerate(title):\n",
        "                data.append({\n",
        "                    'title': title[idx],\n",
        "                    'hyperlinkID': hyperlinkID[idx]\n",
        "                })\n",
        "            return data\n",
        "        else:\n",
        "            return [{\n",
        "                'title':'',\n",
        "                'hyperlinkID': ''\n",
        "            }]\n",
        "\n",
        "    def parseArticle(self, html):\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        reference = soup.find_all('p')\n",
        "        datereference = soup.find_all('div', {'class':'text-right'})\n",
        "        corpus = ''\n",
        "        datetext = ''\n",
        "        if reference:\n",
        "            article = [paragraph.text for paragraph in reference]\n",
        "            corpus = ' '.join(article)\n",
        "\n",
        "            timedata = [t.text for t in datereference]\n",
        "            datetext = ' '.join(timedata)\n",
        "\n",
        "        return {\n",
        "            'corpus':corpus,\n",
        "            'date': datetext\n",
        "        }\n",
        "\n",
        "    def appendToDataFrame(self, title, date, link, corpus):\n",
        "        data = {\n",
        "            'title': title,\n",
        "            'date': date,\n",
        "            'link': link,\n",
        "            'corpus': corpus\n",
        "        }\n",
        "        self.content = self.content.append(data, ignore_index = True)\n",
        "\n",
        "    def show(self, rowNum):\n",
        "        print(self.content.head(rowNum))\n",
        "\n",
        "    def writeFile(self, directory, filename):\n",
        "        if len(self.content):\n",
        "            print(f'Writing results to {directory}/{filename}')\n",
        "            self.content.to_csv(f'/content/{directory}/{filename}')\n",
        "            print('Done')\n",
        "\n",
        "    def saveFile(self):\n",
        "        !zip -r /content/Bernama.zip /content/Bernama\n",
        "        from google.colab import files\n",
        "        files.download(f\"/content/Bernama.zip\")\n",
        "\n",
        "    def work(self, base_url, language, pageNum, verbose):\n",
        "        html = self.fetch(base_url, language, pageNum, 0)\n",
        "        metadatas = self.parseList(html)\n",
        "\n",
        "        if verbose:\n",
        "            for metadata in metadatas:\n",
        "                title = metadata['title']\n",
        "                id = metadata['hyperlinkID']\n",
        "                la = self.defLanguage(language)\n",
        "                url = f'{base_url}/{la}/{id}'\n",
        "\n",
        "                htmlArticle = self.fetch(base_url, language, 0, id)\n",
        "                metaArticle = self.parseArticle(htmlArticle)\n",
        "                datetext = metaArticle['date']\n",
        "                corpus = metaArticle['corpus']\n",
        "\n",
        "                self.appendToDataFrame(title, datetext, url, corpus)\n",
        "        else:\n",
        "            for metadata in metadatas:\n",
        "                title = metadata['title']\n",
        "                id = metadata['hyperlinkID']\n",
        "                la = self.defLanguage(language)\n",
        "                url = f'{base_url}/{la}/{id}'\n",
        "\n",
        "                self.appendToDataFrame(title, '', url, '')\n",
        "\n",
        "    def process(self, base_url, language, directory, verbose, speed, initPage, finalPage):\n",
        "        html = self.fetch(base_url, language, 1, 0)\n",
        "        maxPage = self.obtainLargestPageNum(html)\n",
        "        pageNum = initPage\n",
        "        threadCount = 0\n",
        "        while True:\n",
        "            threads = []\n",
        "            t = threading.Thread(target=self.work, args = (base_url, language, pageNum, verbose))\n",
        "            t.start()\n",
        "            pageNum = pageNum + 1\n",
        "            threadCount = threadCount + 1\n",
        "            threads.append(t)\n",
        "            if pageNum >= finalPage: ######Change this\n",
        "                for thr in threads:\n",
        "                    thr.join()\n",
        "                break\n",
        "            elif threadCount == speed:\n",
        "                threadCount = 0\n",
        "                for thr in threads:\n",
        "                    thr.join()\n",
        "        self.writeFile(directory, 'Bernama.csv')\n",
        "        self.saveFile()\n",
        "\n",
        "    def run(self, speed, initPage, finalPage):\n",
        "        self.create_project(self.directory)\n",
        "        self.process(self.base_url, self.language, self.directory, self.verbose, speed, initPage, finalPage)\n",
        "\n",
        "\n",
        "\n",
        "    def sleep(self, mean, var):\n",
        "        time.sleep(random.random()*var + mean)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0OJsWoJ4fOr"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    base_url = 'https://www.bernama.com'\n",
        "    language = 'en'\n",
        "    directory = 'Bernama'\n",
        "    verbose = True\n",
        "    web_crawler = BernamaScraper(base_url, language, directory, verbose)\n",
        "    web_crawler.run(5, 1635, 1710)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8D_1FjUAkZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67aa0fe-df22-4bc9-f1d0-240d0c9c785a"
      },
      "source": [
        "if len(web_crawler.content):\n",
        "    print(f'Writing results to {directory}/Bernama.csv')\n",
        "    web_crawler.content.to_csv(f'/content/{directory}/Bernama.csv')\n",
        "    print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing results to Bernama/Bernama.csv\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp43apzVIrIS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "68ee7a84-b849-45f5-8207-a9bffe8b0f86"
      },
      "source": [
        "keys = {\n",
        "    \"consumer_key\": 'PFntvVTEGYxOj0DpsQGLfcHih',\n",
        "    \"consumer_secret\": 'qaMWaTihgx7WKCxs2CWf6VkJrGQMaQyBkd22KW6vhdHpRmO7yg',\n",
        "    \"access_token\": '994786472900333570-6qqX2SH8XbHXAbJRkWdwzge4A7xktPo',\n",
        "    \"access_token_secret\": 'ZryyU4n78wJyUQaNVDPNR7X7a7XKDAOtxy768SCMH2Y2E'\n",
        "}\n",
        "\"\"\"\n",
        "MKNJPM, MyHEALTHKKM, kkmm_gov, jpmgov_, KDNPUTRAJAYA,\n",
        "JKJAVMY, DrAdhamBaba, MuhyiddinYassin, DGHisham, IsmailSabri60\n",
        "RTM_Malaysia, AnnuarMusa, 501Awani, bharianmy,\n",
        "hannahyeoh, rafidah72, DrAnwarFazal, MedTweetMYHQ, khairul_hafidz, BuletinTV3\n",
        "ChristinaYong6, loyingru, SarehParangiMD, radzi_dr, ClinicalRsrchMY,\n",
        "Khairykj, SandboxMalaysia, sputnikvaccine, NIHMalaysia, protecthealthco,\n",
        "trussliz, SelveeRamasamy, DrMahHS, malaysianmedic1, angepratt,\n",
        "takeshi_kasai, codebluenews, imokman, DrDzul, HarithIskander,\n",
        "UMonline, fmtoday, SinarOnline, mkini_bm,\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMKNJPM, MyHEALTHKKM, kkmm_gov, jpmgov_, KDNPUTRAJAYA,\\nJKJAVMY, DrAdhamBaba, MuhyiddinYassin, DGHisham, IsmailSabri60\\nRTM_Malaysia, AnnuarMusa, 501Awani, bharianmy,\\nhannahyeoh, rafidah72, DrAnwarFazal, MedTweetMYHQ, khairul_hafidz, BuletinTV3\\nChristinaYong6, loyingru, SarehParangiMD, radzi_dr, ClinicalRsrchMY,\\nKhairykj, SandboxMalaysia, sputnikvaccine, NIHMalaysia, protecthealthco,\\ntrussliz, SelveeRamasamy, DrMahHS, malaysianmedic1, angepratt,\\ntakeshi_kasai, codebluenews, imokman, DrDzul, HarithIskander,\\nUMonline, fmtoday, SinarOnline, mkini_bm, \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA-v9wgIo7E"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
        "from time import sleep\n",
        "import json\n",
        "import datetime\n",
        "## jpmgov_\n",
        "\"\"\"\n",
        "MKNJPM, MyHEALTHKKM, kkmm_gov, jpmgov_, KDNPUTRAJAYA,\n",
        "JKJAVMY, DrAdhamBaba, MuhyiddinYassin, DGHisham, IsmailSabri60\n",
        "RTM_Malaysia, AnnuarMusa, 501Awani, bharianmy,\n",
        "hannahyeoh, rafidah72, DrAnwarFazal, MedTweetMYHQ, khairul_hafidz, BuletinTV3\n",
        "ChristinaYong6, loyingru, SarehParangiMD, radzi_dr, ClinicalRsrchMY,\n",
        "Khairykj, SandboxMalaysia, sputnikvaccine, NIHMalaysia, protecthealthco,\n",
        "trussliz, SelveeRamasamy, DrMahHS, malaysianmedic1, angepratt,\n",
        "takeshi_kasai, codebluenews, imokman, DrDzul, HarithIskander,\n",
        "UMonline, fmtoday, SinarOnline, mkini_bm,\n",
        "\"\"\"\n",
        "# edit these three variables\n",
        "user = 'KDNPUTRAJAYA'\n",
        "start = datetime.datetime(2020, 1, 1)  # year, month, day\n",
        "end = datetime.datetime(2021, 11, 25)  # year, month, day\n",
        "\n",
        "# only edit these if you're having problems\n",
        "delay = 1  # time to wait on each page load before reading the page\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)  # options are Chrome() Firefox() Safari()\n",
        "\n",
        "\n",
        "# don't mess with this stuff\n",
        "twitter_ids_filename = 'all_ids.json'\n",
        "days = (end - start).days + 1\n",
        "id_selector = 'css-4rbku5.css-18t94o4.css-901oao.r-14j79pv.r-1loqt21.r-1q142lx.r-37j5jr.r-a023e6.r-16dba41.r-rjixqe.r-bcqeeo.r-3s2u2q.r-qvutc0'\n",
        "tweet_selector = 'css-901oao.css-16my406.r-poiln3.r-bcqeeo.r-qvutc0'\n",
        "user = user.lower()\n",
        "ids = []\n",
        "\n",
        "def format_day(date):\n",
        "    day = '0' + str(date.day) if len(str(date.day)) == 1 else str(date.day)\n",
        "    month = '0' + str(date.month) if len(str(date.month)) == 1 else str(date.month)\n",
        "    year = str(date.year)\n",
        "    return '-'.join([year, month, day])\n",
        "\n",
        "def form_url(since, until):\n",
        "    p1 = 'https://twitter.com/search?f=tweets&vertical=default&q=(from%3A'\n",
        "    p2 =  user + ')%20since%3A' + since + '%20until%3A' + until + 'include%3Aretweets&src=typd'\n",
        "    return p1 + p2\n",
        "\n",
        "def increment_day(date, i):\n",
        "    return date + datetime.timedelta(days=i)\n",
        "\n",
        "for day in range(days):\n",
        "    d1 = format_day(increment_day(start, 0))\n",
        "    d2 = format_day(increment_day(start, 1))\n",
        "    url = form_url(d1, d2)\n",
        "    print(url)\n",
        "    print(d1)\n",
        "    driver.get(url)\n",
        "    sleep(delay)\n",
        "\n",
        "    try:\n",
        "\n",
        "        #found_tweets = driver.find_elements_by_css_selector(tweet_selector)\n",
        "        SCROLL_PAUSE_TIME = 1\n",
        "\n",
        "        # Get scroll height\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "        while True:\n",
        "            # Wait to load page\n",
        "            time.sleep(SCROLL_PAUSE_TIME/4)\n",
        "            # Scroll down to bottom\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "            # Wait to load page\n",
        "            time.sleep(SCROLL_PAUSE_TIME)\n",
        "            counter = 0\n",
        "            try:\n",
        "            #id = tweet.find_element_by_class_name(id_selector).get_attribute('href').split('/')[-1]\n",
        "            #idlst = driver.find_elements(By.CLASS_NAME, id_selector)\n",
        "                idlst = driver.find_elements(By.TAG_NAME,'a')\n",
        "                time.sleep(SCROLL_PAUSE_TIME/4)\n",
        "                for id in idlst:\n",
        "                    link = ''\n",
        "                    try:\n",
        "                        link = (id.get_attribute('href'))\n",
        "\n",
        "                        if 'status' in link and not 'photo' in link:\n",
        "                            ids.append(link.split('/')[-1])\n",
        "                            counter = counter + 1\n",
        "                    except:\n",
        "                        print('NA')\n",
        "                print('{} found'.format(counter))\n",
        "            except StaleElementReferenceException as e:\n",
        "                print('lost element reference', tweet)\n",
        "\n",
        "\n",
        "            # Calculate new scroll height and compare with last scroll height\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "        time.sleep(2)\n",
        "        #print(driver.page_source)\n",
        "        #found_tweets = driver.find_elements(By.CLASS_NAME, tweet_selector)\n",
        "\n",
        "        \"\"\"\n",
        "        increment = 10\n",
        "        #print(found_tweets)\n",
        "        while len(found_tweets) >= increment:\n",
        "            print('scrolling down to load more tweets')\n",
        "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
        "            sleep(delay)\n",
        "            found_tweets.append(driver.find_elements(By.CLASS_NAME, tweet_selector))\n",
        "            increment += 10\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        #print('{} tweets found'.format(len(found_tweets)))\n",
        "\n",
        "    except NoSuchElementException:\n",
        "        print('no tweets on this day')\n",
        "\n",
        "    start = increment_day(start, 1)\n",
        "\n",
        "\"\"\"\n",
        "try:\n",
        "    with open(twitter_ids_filename) as f:\n",
        "        all_ids = ids + json.load(f)\n",
        "        data_to_write = list(set(all_ids))\n",
        "        print('tweets found on this scrape: ', len(ids))\n",
        "        print('total tweet count: ', len(data_to_write))\n",
        "except FileNotFoundError:\n",
        "    with open(twitter_ids_filename, 'w') as f:\n",
        "        all_ids = ids\n",
        "        data_to_write = list(set(all_ids))\n",
        "        print('tweets found on this scrape: ', len(ids))\n",
        "        print('total tweet count: ', len(data_to_write))\n",
        "\n",
        "with open(twitter_ids_filename, 'w') as outfile:\n",
        "    json.dump(data_to_write, outfile)\n",
        "\"\"\"\n",
        "with open(twitter_ids_filename, 'w') as outfile:\n",
        "    json.dump(list(set(ids)), outfile)\n",
        "print('all done here')\n",
        "driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxPb1hoRIvCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8633c819-7f7b-4ef0-c9fa-a98660e146ed"
      },
      "source": [
        "import tweepy\n",
        "import json\n",
        "import math\n",
        "import glob\n",
        "import csv\n",
        "import zipfile\n",
        "import zlib\n",
        "from tweepy import TweepError\n",
        "from time import sleep\n",
        "\n",
        "# CHANGE THIS TO THE USER YOU WANT\n",
        "user = 'KDNPUTRAJAYA'\n",
        "\"\"\"\n",
        "with open('api_keys.json') as f:\n",
        "    keys = json.load(f)\n",
        "\"\"\"\n",
        "auth = tweepy.OAuthHandler(keys['consumer_key'], keys['consumer_secret'])\n",
        "auth.set_access_token(keys['access_token'], keys['access_token_secret'])\n",
        "api = tweepy.API(auth)\n",
        "user = user.lower()\n",
        "output_file = '{}.json'.format(user)\n",
        "output_file_short = '{}_short.json'.format(user)\n",
        "compression = zipfile.ZIP_DEFLATED\n",
        "\n",
        "with open('all_ids.json') as f:\n",
        "    ids = json.load(f)\n",
        "\n",
        "print('total ids: {}'.format(len(ids)))\n",
        "\n",
        "all_data = []\n",
        "start = 0\n",
        "end = 100\n",
        "limit = len(ids)\n",
        "i = math.ceil(limit / 100)\n",
        "\n",
        "for go in range(i):\n",
        "    print('currently getting {} - {}'.format(start, end))\n",
        "    sleep(6)  # needed to prevent hitting API rate limit\n",
        "    id_batch = ids[start:end]\n",
        "    start += 100\n",
        "    end += 100\n",
        "    tweets = api.statuses_lookup(id_batch)\n",
        "    for tweet in tweets:\n",
        "        all_data.append(dict(tweet._json))\n",
        "print(all_data)\n",
        "print('metadata collection complete')\n",
        "print('creating master json file')\n",
        "with open(output_file, 'w') as outfile:\n",
        "    json.dump(all_data, outfile)\n",
        "\n",
        "print('creating ziped master json file')\n",
        "zf = zipfile.ZipFile('{}.zip'.format(user), mode='w')\n",
        "zf.write(output_file, compress_type=compression)\n",
        "zf.close()\n",
        "\n",
        "results = []\n",
        "\n",
        "def is_retweet(entry):\n",
        "    return 'retweeted_status' in entry.keys()\n",
        "\n",
        "def get_source(entry):\n",
        "    s = f'https://twitter.com/{user}/status/{entry}'\n",
        "    return s\n",
        "\n",
        "with open(output_file) as json_data:\n",
        "    data = json.load(json_data)\n",
        "    for entry in data:\n",
        "        t = {\n",
        "            \"created_at\": entry[\"created_at\"],\n",
        "            \"text\": entry[\"text\"],\n",
        "            \"in_reply_to_screen_name\": entry[\"in_reply_to_screen_name\"],\n",
        "            \"retweet_count\": entry[\"retweet_count\"],\n",
        "            \"favorite_count\": entry[\"favorite_count\"],\n",
        "            \"source\": get_source(entry[\"id_str\"]),\n",
        "            \"id_str\": entry[\"id_str\"],\n",
        "            \"is_retweet\": is_retweet(entry)\n",
        "        }\n",
        "        results.append(t)\n",
        "\n",
        "print('creating minimized json master file')\n",
        "with open(output_file_short, 'w') as outfile:\n",
        "    json.dump(results, outfile)\n",
        "\n",
        "with open(output_file_short) as master_file:\n",
        "    data = json.load(master_file)\n",
        "    #fields = [\"favorite_count\", \"source\", \"text\", \"in_reply_to_screen_name\", \"is_retweet\", \"created_at\", \"retweet_count\", \"id_str\"]\n",
        "    fields = [\"favorite_count\", \"source\", \"text\", \"created_at\", \"retweet_count\", \"id_str\"]\n",
        "    print('creating CSV version of minimized json master file')\n",
        "    with open('{}.csv'.format(user), 'w') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "        writer.writeheader()\n",
        "        for x in results:\n",
        "            #f.writerow([x[\"favorite_count\"], x[\"source\"], x[\"text\"], x[\"in_reply_to_screen_name\"], x[\"is_retweet\"], x[\"created_at\"], x[\"retweet_count\"], x[\"id_str\"]])\n",
        "\n",
        "            small_dict = { your_key: x[your_key] for your_key in fields }\n",
        "            writer.writerow(small_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total ids: 1037\n",
            "currently getting 0 - 100\n",
            "currently getting 100 - 200\n",
            "currently getting 200 - 300\n",
            "currently getting 300 - 400\n",
            "currently getting 400 - 500\n",
            "currently getting 500 - 600\n",
            "currently getting 600 - 700\n",
            "currently getting 700 - 800\n",
            "currently getting 800 - 900\n",
            "currently getting 900 - 1000\n",
            "currently getting 1000 - 1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating ziped master json file\n",
            "creating minimized json master file\n",
            "creating CSV version of minimized json master file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4cU9oLgkHo",
        "outputId": "9c7a6adc-981b-43f1-d1dd-083e4d78a89b"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating CSV version of minimized json master file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66eGSC4FcNQx"
      },
      "source": [
        "for data in all_data:\n",
        "    print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtbC_MWEU668",
        "outputId": "83f13373-26ee-4b7c-95a0-62b0729456ec"
      },
      "source": [
        "text_query = '(from:Khairykj) until:2021-12-31 since:2020-01-01'\n",
        "try:\n",
        " # Creation of query method using parameters\n",
        "    tweets = tweepy.Cursor(api.search,q=text_query).item(150)\n",
        "\n",
        "    # Pulling information from tweets iterable object\n",
        "    outtweets = [[tweet.id_str,\n",
        "                    tweet.created_at,\n",
        "                    str(f'https:/twitter.com/{userID}/status/{tweet.id_str}'),\n",
        "                    tweet.favorite_count,\n",
        "                    tweet.retweet_count,\n",
        "                    tweet.full_text.encode(\"utf-8\").decode(\"utf-8\")]\n",
        "                    for idx,tweet in enumerate(tweets)]\n",
        "    dt = {'id':str, \"created_at\": str, \"link\": str, \"favorite_count\": str, \"retweet_count\":str, \"text\":str }\n",
        "    df = pd.DataFrame(outtweets,columns=[\"id\",\"created_at\",\"link\",\"favorite_count\",\"retweet_count\", \"text\"])\n",
        "    df.astype(dt)\n",
        "    df.to_csv('%s_tweets.csv' % userID, index=True)\n",
        "\n",
        "except BaseException as e:\n",
        "    print('failed on_status,',str(e))\n",
        "    time.sleep(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed on_status, 'Cursor' object has no attribute 'item'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDJVNPx3Y5at",
        "outputId": "21f63e37-11b1-4ffe-dd91-00e02480b6ab"
      },
      "source": [
        "tweets = []\n",
        "for status in tweepy.Cursor(api.user_timeline, id='DGHisham').items():\n",
        "    tweets.append(status)\n",
        "\n",
        "print(len(tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "DbOYI4FKauGO",
        "outputId": "b068c286-79e8-4e90-fcc4-1eec879f1811"
      },
      "source": [
        "curl https://api.twitter.com/2/tweets/search/recent?query=cat%20has%3Amedia%20-grumpy&tweet.fields=created_at&max_results=100 -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-70c515fdc1eb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    curl https://api.twitter.com/2/tweets/search/recent?query=cat%20has%3Amedia%20-grumpy&tweet.fields=created_at&max_results=100 -H \"Authorization: Bearer $BEARER_TOKEN\"\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}