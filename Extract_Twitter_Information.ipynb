{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "co34Jb0817Ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu1dNVG31bfx"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "!pip install requests\n",
        "!pip3 install snscrape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Modules"
      ],
      "metadata": {
        "id": "JiC-uA651__4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import snscrape.modules.telegram as sntelegram\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import tweepy as tw\n",
        "import random\n",
        "import datetime\n",
        "import threading\n",
        "import glob\n",
        "import zipfile\n",
        "import zlib\n",
        "import multiprocessing\n",
        "from tweepy import TweepError"
      ],
      "metadata": {
        "id": "EgVAxa-s16_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/sample_data')\n"
      ],
      "metadata": {
        "id": "dUbTuM-eK3xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter"
      ],
      "metadata": {
        "id": "ZCqnuoTj2Fx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consumer_key= 'PFntvVTEGYxOj0DpsQGLfcHih'\n",
        "consumer_secret= 'qaMWaTihgx7WKCxs2CWf6VkJrGQMaQyBkd22KW6vhdHpRmO7yg'\n",
        "access_token= '994786472900333570-6qqX2SH8XbHXAbJRkWdwzge4A7xktPo'\n",
        "access_token_secret= 'ZryyU4n78wJyUQaNVDPNR7X7a7XKDAOtxy768SCMH2Y2E'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "5h3Mo5Dk1uGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "begin_date = \"2015-01-01\"\n",
        "end_date = \"2023-02-04\"\n",
        "max_tweets = 100000\n",
        "counter = 1"
      ],
      "metadata": {
        "id": "UMjfkofs2Kqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = ['_AmranFans']"
      ],
      "metadata": {
        "id": "QC2q5Lcj2Ocn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for user in targets:\n",
        "    # Creating list to append tweet data\n",
        "    tweets_list = []\n",
        "    id_lst = []\n",
        "    print(f'Now extracting tweets from {user}, {counter}/{len(targets)}')\n",
        "    search_key = f'(from:{user}) include:nativeretweets until:{end_date} since:{begin_date}'\n",
        "    try:\n",
        "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(search_key).get_items()):\n",
        "            if i > max_tweets:\n",
        "                break\n",
        "            elif i % 1000 == 0:\n",
        "                print(f'{i} tweets have been scraped')\n",
        "            tweets_list.append([tweet.date, tweet.id, tweet.lang, tweet.source,\n",
        "                                tweet.url, tweet.rawContent, tweet.user.username])\n",
        "            id_lst.append(tweet.id)\n",
        "    except:\n",
        "        print(\"Protected account!\")\n",
        "        pass\n",
        "    # Creating a dataframe from the tweets list above\n",
        "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Lang',\n",
        "                                                   'Source', 'URL', 'Text',\n",
        "                                                   'Username'])\n",
        "    if len(tweets_df) > 0:\n",
        "        tweets_df.to_csv(f'{user.lower()}.csv')\n",
        "        with open(f'{user.lower()}.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(json.dumps(id_lst), f, ensure_ascii=False, indent=4)\n",
        "    counter = counter + 1"
      ],
      "metadata": {
        "id": "EaUrrGfX2M8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def searchProfile(userhandle, begin_date=None, end_date=None, max_tweets=None):\n",
        "    if not isinstance(userhandle, (str)):\n",
        "        raise ValueError(\"The argument userhandle must be a string.\")\n",
        "    if not isinstance(begin_date, (str, type(None))):\n",
        "        raise ValueError(\"The argument begin_date must be a string.\")\n",
        "    if not isinstance(end_date, (str, type(None))):\n",
        "        raise ValueError(\"The argument end_date must be a string.\")\n",
        "    if not isinstance(max_tweets, (int, type(None))):\n",
        "        raise ValueError(\"The argument max_tweets must be an Int.\")\n",
        "\n",
        "    if begin_date is None:\n",
        "        begin_date='2006-03-21'\n",
        "    if end_date is None:\n",
        "        end_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "    print(f'Now extracting tweets from @{userhandle}\\n')\n",
        "\n",
        "    tweets_list = []\n",
        "    id_lst = []\n",
        "\n",
        "    try:\n",
        "        err = 0\n",
        "        for i, tweet in enumerate(sntwitter.TwitterUserScraper(userhandle).get_items()):\n",
        "            if max_tweets is not None:\n",
        "                if i > max_tweets:\n",
        "                    break\n",
        "\n",
        "            if i % 1000 == 0 and i != 0:\n",
        "                print(f'{i} tweets have been scraped from user @{userhandle}\\n')\n",
        "\n",
        "            try:\n",
        "                tweets_list.append([tweet.date, tweet.id, tweet.lang, tweet.source,\n",
        "                                    tweet.url, tweet.rawContent, tweet.renderedContent,\n",
        "                                    tweet.likeCount,\n",
        "                                    tweet.replyCount, tweet.retweetCount,\n",
        "                                    tweet.quoteCount,tweet.viewCount,\n",
        "                                    tweet.hashtags, tweet.inReplyToTweetId,\n",
        "                                    tweet.inReplyToUser,\n",
        "                                    tweet.quotedTweet,\n",
        "                                    tweet.user.username])\n",
        "\n",
        "                id_lst.append(tweet.id)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(f'While extracting tweets from @{userhandle}, an error has occured.')\n",
        "                err = err + 1\n",
        "                print(f'While extracting tweets from @{userhandle}, total number of error has increased to {err}')\n",
        "\n",
        "\n",
        "        print(f\"Total of {i - err} tweets have been scraped from @{userhandle}\")\n",
        "        # Creating a dataframe from the tweets list above\n",
        "        tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Lang',\n",
        "                                                    'Source', 'URL', 'Text', 'Rendered Content',\n",
        "                                                    'Like Count',\n",
        "                                                    'Reply Count', 'Retweet Count', 'Quote Count',\n",
        "                                                    'View Count', 'Hashtags',\n",
        "                                                    'inReplyToTweetId',\n",
        "                                                    'inReplyToUser',\n",
        "                                                    'quotedTweet',\n",
        "                                                    'Username'])\n",
        "        if len(tweets_df) > 0:\n",
        "            tweets_df.to_csv(f'{tweet.user.username.lower()}.csv')\n",
        "            with open(f'{tweet.user.username.lower()}.json', 'w', encoding='utf-32') as f:\n",
        "                json.dump(json.dumps(id_lst), f, ensure_ascii=False, indent=4)\n",
        "    except:\n",
        "        print(f\"@{userhandle} is a protected account!\")\n",
        "        pass\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AXgpsZFw94TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multipleSearchProfile(userhandles, processes_num=1):\n",
        "\n",
        "    if processes_num <= 0:\n",
        "        processes_num = len(userhandles)\n",
        "\n",
        "    with multiprocessing.Pool(processes=processes_num) as pool:\n",
        "        pool.starmap(searchProfile, [(item,) for item in userhandles])\n",
        "\n"
      ],
      "metadata": {
        "id": "X6luW5OcA2Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = r'/content/userhandles.txt'\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    userhandles = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ZzWNXEhuvVSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multipleSearchProfile(userhandles, processes_num=0)"
      ],
      "metadata": {
        "id": "euyC1zbISImE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content\n",
        "files.download(r'/content/')"
      ],
      "metadata": {
        "id": "s-a8zi5TOhi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Telegram"
      ],
      "metadata": {
        "id": "Y99Vc7NH2LB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "telegram_channel_name = 'greatreject'\n",
        "data = sntelegram.TelegramChannelScraper(telegram_channel_name).get_items()"
      ],
      "metadata": {
        "id": "_GOTW7SP2OzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for d in data:\n",
        "    if i > 10:\n",
        "        break\n",
        "    i = i + 1\n",
        "    print(f\"{d.url} {d.date}\\n{d.content}\\n{d.outlinks}\\n{d.linkPreview}\")\n",
        "    print('-'*8)"
      ],
      "metadata": {
        "id": "61XR8ViV2yjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}