{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Patents View"
      ],
      "metadata": {
        "id": "cSy_wDowdTUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cssselect\n",
        "!pip install requests_html\n",
        "!pip install nest-asyncio\n",
        "!pip install pypatent"
      ],
      "metadata": {
        "id": "42LbxM1aGoea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4AD8gn5KCal",
        "outputId": "95494ffc-806a-4396-9ab1-1b33af7f07a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRO1BbWt8brr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pypatent\n",
        "import pandas as pd\n",
        "import json\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from requests_html import HTML\n",
        "from requests_html import HTMLSession\n",
        "from requests_html import AsyncHTMLSession\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import secrets\n",
        "from bs4 import BeautifulSoup\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://api.patentsview.org/patents/query?q={\"_and\":[{\"_gte\":{\"patent_date\":\"2006-01-01\"}},{\"_text_any\":{\"patent_abstract\":\"hydrogen\"}},{\"_text_any\":{\"patent_title\":\"hydrogen\"}},{\"assignee_lastknown_country\":\"US\"}]}&f=[\"patent_number\", \"patent_title\", \"patent_year\", \"patent_num_claims\", \"patent_abstract\"]&o={\"page\":1,\"per_page\":100}'\n",
        "response = requests.get(url, timeout=30)\n",
        "print(response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmbZ9yF08pFp",
        "outputId": "dd5c2ed6-5056-408d-c446-8d3ff9b3834d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = response.json()\n",
        "data"
      ],
      "metadata": {
        "id": "kQ2F9da-9Fx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_source(url):\n",
        "    \"\"\"Return the source code for the provided URL.\n",
        "\n",
        "    Args:\n",
        "        url (string): URL of the page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        response (object): HTTP response object from requests_html.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        session = HTMLSession()\n",
        "        response = session.get(url)\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(e)\n",
        "\n",
        "def scrape_google(query):\n",
        "\n",
        "    query = urllib.parse.quote_plus(query)\n",
        "    response = get_source(\"https://www.google.co.uk/search?q=\" + query)\n",
        "\n",
        "    links = list(response.html.absolute_links)\n",
        "    google_domains = ('https://www.google.',\n",
        "                      'https://google.',\n",
        "                      'https://webcache.googleusercontent.',\n",
        "                      'http://webcache.googleusercontent.',\n",
        "                      'https://policies.google.',\n",
        "                      'https://support.google.',\n",
        "                      'https://maps.google.')\n",
        "\n",
        "    for url in links[:]:\n",
        "        if url.startswith(google_domains):\n",
        "            links.remove(url)\n",
        "\n",
        "    return links\n",
        "\n",
        "def get_results(query):\n",
        "\n",
        "    query = urllib.parse.quote_plus(query)\n",
        "    response = get_source(\"https://www.google.co.uk/search?q=\" + query)\n",
        "\n",
        "    return response\n",
        "\n",
        "def parse_results(response):\n",
        "\n",
        "    css_identifier_result = \".tF2Cxc\"\n",
        "    css_identifier_title = \"h3\"\n",
        "    css_identifier_link = \".yuRUbf a\"\n",
        "    css_identifier_text = \".VwiC3b\"\n",
        "\n",
        "    results = response.html.find(css_identifier_result)\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        item = {\n",
        "            'title': result.find(css_identifier_title, first=True).text,\n",
        "            'link': result.find(css_identifier_link, first=True).attrs['href'],\n",
        "            'text': result.find(css_identifier_text, first=True).text\n",
        "        }\n",
        "\n",
        "        output.append(item)\n",
        "\n",
        "    return output\n",
        "\n",
        "def google_search(query):\n",
        "    response = get_results(query)\n",
        "    return parse_results(response)"
      ],
      "metadata": {
        "id": "j6s1AUvq--zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scrape_google(\"data science blogs\")\n",
        "\n",
        "results = google_search(\"web scraping\")\n",
        "results"
      ],
      "metadata": {
        "id": "42_DhEBtGvbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Search Common Crawl"
      ],
      "metadata": {
        "id": "d8lhmtsU2aDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdx_toolkit"
      ],
      "metadata": {
        "id": "VHUn4cGu5HCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cdx_toolkit\n",
        "cdx = cdx_toolkit.CDXFetcher(source='cc')\n",
        "url='https://www.reddit.com/r/dataisbeautiful/*'\n",
        "objs = list(cdx.iter(url, from_ts='202002', to='202006',\n",
        "                     limit=1000, filter='=status:200'))\n",
        "[o.data for o in objs]"
      ],
      "metadata": {
        "id": "dvvEHB6z5qzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import io\n",
        "import gzip\n",
        "import csv\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "\n",
        "domain = 'cnn.com'\n",
        "\n",
        "# list of available indices\n",
        "index_list = [\"2014-52\",\"2015-06\",\"2015-11\",\"2015-14\",\"2015-18\",\"2015-22\",\"2015-27\"]\n",
        "\n",
        "#\n",
        "# Searches the Common Crawl Index for a domain.\n",
        "#\n",
        "def search_domain(domain):\n",
        "\n",
        "    record_list = []\n",
        "\n",
        "    print(\"[*] Trying target domain: %s\" % domain)\n",
        "\n",
        "    for index in index_list:\n",
        "\n",
        "        print(\"[*] Trying index %s\" % index)\n",
        "\n",
        "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
        "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
        "\n",
        "        response = requests.get(cc_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            records = response.content.splitlines()\n",
        "\n",
        "            for record in records:\n",
        "                record_list.append(json.loads(record))\n",
        "\n",
        "            print(\"[*] Added %d results.\" % len(records))\n",
        "\n",
        "\n",
        "    print(\"[*] Found a total of %d hits.\" % len(record_list))\n",
        "\n",
        "    return record_list\n",
        "\n",
        "#\n",
        "# Downloads a page from Common Crawl - adapted graciously from @Smerity - thanks man!\n",
        "# https://gist.github.com/Smerity/56bc6f21a8adec920ebf\n",
        "#\n",
        "def download_page(record):\n",
        "\n",
        "    offset, length = int(record['offset']), int(record['length'])\n",
        "    offset_end = offset + length - 1\n",
        "\n",
        "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
        "    # Getting the file on S3 is equivalent however - you can request a Range\n",
        "    prefix = 'https://aws-publicdatasets.s3.amazonaws.com/'\n",
        "\n",
        "    # We can then use the Range header to ask for just this set of bytes\n",
        "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
        "\n",
        "    # The page is stored compressed (gzip) to save space\n",
        "    # We can extract it using the GZIP library\n",
        "    raw_data = StringIO(resp.content.decode('utf-8'))\n",
        "    f = gzip.GzipFile(fileobj=raw_data)\n",
        "\n",
        "    # What we have now is just the WARC response, formatted:\n",
        "    data = f.read()\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    if len(data):\n",
        "        try:\n",
        "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return response\n",
        "\n",
        "#\n",
        "# Extract links from the HTML\n",
        "#\n",
        "def extract_external_links(html_content,link_list):\n",
        "\n",
        "    parser = BeautifulSoup(html_content)\n",
        "\n",
        "    links = parser.find_all(\"a\")\n",
        "\n",
        "    if links:\n",
        "\n",
        "        for link in links:\n",
        "            href = link.attrs.get(\"href\")\n",
        "\n",
        "            if href is not None:\n",
        "\n",
        "                if domain not in href:\n",
        "                    if href not in link_list and href.startswith(\"http\"):\n",
        "                        print(\"[*] Discovered external link: %s\" % href)\n",
        "                        link_list.append(href)\n",
        "\n",
        "    return link_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9j6DJBzT2eyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_list = search_domain(domain)"
      ],
      "metadata": {
        "id": "Ihv5GGJq3oUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_list   = []\n",
        "\n",
        "for record in record_list:\n",
        "\n",
        "    html_content = download_page(record)\n",
        "\n",
        "    print(\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
        "\n",
        "    link_list = extract_external_links(html_content,link_list)\n",
        "\n",
        "\n",
        "print(\"[*] Total external links discovered: %d\" % len(link_list))\n",
        "\n",
        "with codecs.open(\"%s-links.csv\" % domain,\"wb\",encoding=\"utf-8\") as output:\n",
        "\n",
        "    fields = [\"URL\"]\n",
        "\n",
        "    logger = csv.DictWriter(output,fieldnames=fields)\n",
        "    logger.writeheader()\n",
        "\n",
        "    for link in link_list:\n",
        "        logger.writerow({\"URL\":link})"
      ],
      "metadata": {
        "id": "NLMGdm5B4lCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawler Common"
      ],
      "metadata": {
        "id": "e98VRUOQCXcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import io\n",
        "import gzip\n",
        "import csv\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        ""
      ],
      "metadata": {
        "id": "tZ1iOgjiCaL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_domain(domain):\n",
        "\n",
        "    record_list = []\n",
        "\n",
        "    print(\"[*] Trying target domain: %s\" % domain)\n",
        "\n",
        "    for index in index_list:\n",
        "\n",
        "        print(\"[*] Trying index %s\" % index)\n",
        "\n",
        "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
        "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
        "\n",
        "        response = requests.get(cc_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            records = response.content.splitlines()\n",
        "\n",
        "            for record in records:\n",
        "                record_list.append(json.loads(record))\n",
        "\n",
        "            print(\"[*] Added %d results.\" % len(records))\n",
        "\n",
        "\n",
        "    print(\"[*] Found a total of %d hits.\" % len(record_list))\n",
        "\n",
        "    return record_list"
      ],
      "metadata": {
        "id": "LKZNRWxBCgXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_page(record):\n",
        "\n",
        "    offset, length = int(record['offset']), int(record['length'])\n",
        "    offset_end = offset + length - 1\n",
        "\n",
        "    prefix = 'https://data.commoncrawl.org/'\n",
        "\n",
        "    resp = requests.get(prefix + record_list[0]['filename'],\n",
        "                        headers={'Range': 'bytes={}-{}'.format(offset,\n",
        "                                                               offset_end)})\n",
        "\n",
        "    if resp.status_code != 200 and resp.status_code != 206:\n",
        "        print(f\"{resp.status_code}: {prefix + record_list[0]['filename']}\")\n",
        "        return\n",
        "\n",
        "    data = resp.content\n",
        "    try:\n",
        "        raw_data = gzip.decompress(data)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    if len(data):\n",
        "        try:\n",
        "            warc, header, response = str(raw_data).strip().split(r'\\r\\n\\r\\n', 2)\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return response\n",
        "\n"
      ],
      "metadata": {
        "id": "dJ7uF4UMCkik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_external_links(html_content,link_list):\n",
        "\n",
        "    parser = BeautifulSoup(html_content)\n",
        "\n",
        "    links = parser.find_all(\"a\")\n",
        "\n",
        "    if links:\n",
        "\n",
        "        for link in links:\n",
        "            href = link.attrs.get(\"href\")\n",
        "\n",
        "            if href is not None:\n",
        "\n",
        "                if domain not in href:\n",
        "                    if href not in link_list and href.startswith(\"http\"):\n",
        "                        print(\"[*] Discovered external link: %s\" % href)\n",
        "                        link_list.append(href)\n",
        "\n",
        "    return link_list\n"
      ],
      "metadata": {
        "id": "R_TbXm14EVEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain = 'cnn.com'\n",
        "\n",
        "# list of available indices\n",
        "index_list = [\"2014-52\",\"2015-06\",\"2015-11\",\"2015-14\",\"2015-18\",\"2015-22\",\"2015-27\"]"
      ],
      "metadata": {
        "id": "Fd9UP3yLCjsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_list = search_domain(domain)"
      ],
      "metadata": {
        "id": "RcvodDxVEYhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wet_url = record_list[0]['filename'].replace('/warc/', '/wet/').replace('warc.gz', 'warc.wet.gz')"
      ],
      "metadata": {
        "id": "xR-iXESqIntL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.get('https://data.commoncrawl.org/' + wet_url)\n",
        "print(resp.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KIFoNwyI8ET",
        "outputId": "8f843780-1105-44ec-9e65-91c95961f6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = resp.content\n",
        "try:\n",
        "    raw_data = gzip.decompress(data)\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "387xE0H-JMbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wet, header, response = str(raw_data).strip().split(r'\\r\\n\\r\\n', 2)"
      ],
      "metadata": {
        "id": "_I60l5-yJURR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/WhisperVideo/test.txt', 'w') as f:\n",
        "    f.write(response)"
      ],
      "metadata": {
        "id": "ht5IJUbhJaWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_list   = []\n",
        "\n",
        "for idx, record in enumerate(record_list):\n",
        "\n",
        "    if idx >= 1:\n",
        "        break\n",
        "\n",
        "    html_content = download_page(record)\n",
        "\n",
        "    if html_content is None:\n",
        "        continue\n",
        "\n",
        "    print(\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
        "\n",
        "    link_list = extract_external_links(html_content,link_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "xAlkZgihEwJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with codecs.open(\"%s-links.csv\" % domain,\"wb\",encoding=\"utf-8\") as output:\n",
        "\n",
        "    fields = [\"URL\"]\n",
        "\n",
        "    logger = csv.DictWriter(output,fieldnames=fields)\n",
        "    logger.writeheader()\n",
        "\n",
        "    for link in link_list:\n",
        "        logger.writerow({\"URL\":link})"
      ],
      "metadata": {
        "id": "deiChvsIF9Xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}